

38 | 案例分析（一）：高性能限流器Guava RateLimiter

从今天开始，我们就进入案例分析模块了。这个模块我们将分析四个经典的开源框架，看看它们
是如何处理并发问题的，通过这四个案例的学习，相信你会对如何解决并发问题有个更深入
的认识。

首先 guava  RateLimiter 是如何解决高并发场景下的限流问题的。guava是
google开源的类库，提供了一个工具类RateLimiter .我们先来看看 RateLimiter的使用
，让你对限流有个感官上的印象。假设我们有一个线程池，它每秒只能处理两个任务，如果提交
的任务过快，可能导致系统不稳定，这个时候就需要用到限流。

在下面的实例代码中，我们创建了一个流速为2个请求每秒的限流器，这个里的流速该怎么理解呢
直观地看，2个请求每秒值得是每秒最多允许2个请求通过限流器。其实在guava中
流速还有更深层的意思：是一种匀速的概念，2个请求每秒等价于1个请求500毫秒。

经典限流算法：令牌桶算法
guava的限流器使用上还是很简单的,那它是如何实现的呢？guava采用的是令牌桶算法，
其核心是要想通过限流器，必须拿到令牌。也就是说，只要我们能够限制发放令牌的速率，那么
就能控制流速了。令牌桶算法的详细描述如下：
1.令牌以固定的速率添加到令牌桶中，假设限流的速率是R/秒，则令牌每1/r秒会添加一个；

2.假设令牌桶的容量是b，如果令牌桶已满，则新的令牌会被丢弃；

3.请求能够通过限流器的前提是令牌桶中有令牌。

这个算法中，限流的速率r还是比较容易理解的，但令牌桶的容量b该怎么理解呢？b其实是
burst的简写，意义是限流器允许的最大突发流量。比如b=10,而且令牌桶中的令牌已满，
此时限流器允许10个请求同事通过限流器，当然只是突发流量而已，这10个请求会带走10
个令牌，所以后续的流量只能按照速率r通过限流器。

令牌桶这个算法，如何用java实现呢？很可能你的直接会告诉你生产者-消费者模式：一个
生产者线程定时向阻塞队列中添加令牌，而试图通过限流器的线程则作为消费者线程，只有从
阻塞队列中获取到令牌，才运行通过限流器。

这个算法看上去非常完美，而且实现起来非常简单，如果并发量不打，这个实现并没有什么问题
。可实际情况却是使用限流的常军大部分都是高并发常军，而且系统压力已经临近极限了，
此时这个实现就有问题了。问题就出在定时器上，在高并发场景下，当系统压力已经临近极限的时候
，定时器的精度误差会非常大，同时定时器本身会创建调度线程，也会对系统的性能产生
影响。

那还有什么好的实现方式呢？当然有，guava的实现就没有使用定时器，下面我们就来看看它是
如何实现的。


Guava如何实现令牌桶算法
guava实现令牌桶算法，用了一个很简单的办法，其关键是记录并动态计算下一令牌发放的时间
。下面我们以一个最简单的常军来介绍该算法的执行过程。假设令牌桶的容量为B=1，限流
速率R=1个请求秒 如下图所示，如果当前令牌桶中没有令牌，下一个令牌的发放时间是在
第三秒，而在第二秒的时候有一个线程T1请求令牌，此时该如何处理呢？

对于这个请求令牌的线程而言，很先让需要等待1秒，因为1秒以后它就能拿到
令牌了。此时需要注意的是，下一个令牌发放的时间也要增加1秒，为什么呢？因为第三秒
发放的令牌已经被线程T1预占了。处理之后如下图所示。

假设T1在预占了第三秒的令牌之后，马上又有一个线程T2请求令牌，如下图所示。

很显然，由于下一个令牌产生的时间是第四秒，所以线程T2要等待两秒的时间，才能获取到
令牌，同时由于T2预占了第四秒的令牌，所以下一令牌产生还要增加1秒，完全处理之后，
如下图所示。

上面线程T1 T2都是在下一令牌产生时间之前请求令牌，如果线程在下一令牌产生时间之后
请求令牌会如何呢？假设在线程T1请求令牌之后的5秒，也就是第七秒，线程T3请求令牌
如下图所示。

由于在第五秒已经产生了一个令牌，所以此时线程T3可以直接拿到令牌，而无需等待。在第
7秒，实际上限流器能够产生3个令牌，第567秒各产生一个令牌。由于我们假设令牌
桶的容量是1，所以第67秒产生的令牌就丢弃了，其实等价地你可以认为是保留的第四秒的令牌
，丢弃的第56秒的令牌，也就是说第7秒的令牌呗线程T3占有了，于是下一令牌
的的产生时间应该是第8秒，如下图所示。



通过上面简要地分析，你会发现，我们只需要记录一个下一令牌产生的时间，并动态更新它，
就能够轻松完成限流功能。我们可以将上面的这个算法代码化，实例代码如下所示，依然假设
令牌桶的容量是1.关键 reserve 方法，这个方法会为请求令牌的线程预分配令牌，同时
返回该线程能够获取令牌的时间。其实现逻辑就是上面提到的：如果线程请求令牌的时间在下一
令牌产生时间之后，那么该线程立刻就能够获取令牌；反之，如果请求时间在下一令牌产生
时间之前，那么该线程是在下一令牌产生的时间获取令牌。由于此时下一令牌已经被该线程预先占有
，所以下一令牌产生的时间需要加上1秒。
reserve
acquire

SimpleLimiter

如果令牌桶的容量大于1，又该如何处理呢？按照令牌桶算法，令牌要首先从令牌桶中出，所以
我们需要按需计算令牌桶中的数量，当有线程请求令牌时，先从令牌桶中出。具体的代码实现
如下所示。我们增加了一个 resync 方法，在这个方法中，如果线程请求令牌的时间在下
一令牌产生时间之后，会重新计算令牌桶中的领牌数，新产生的令牌的计算公式是：now-next/interval
你可对照上面的示意图来理解。 reserve 方法中，则增加了先从令牌桶中出
令牌的逻辑，不过需要注意的是，如果令牌是从令牌桶中出的，那么next就无需增加一个
interval了。




总结

经典的限流算法有两个，一个是令牌桶算法 token bucket ，另一个是漏桶算法 leaky bucket.
令牌桶算法是定时向令牌桶发送令牌，请求能够从令牌桶中拿到令牌，然后才能
通过限流器；而漏铜算法里，请求就像水一样注入漏桶，漏桶会按照一定的速率自动将水漏掉，
只有漏桶里还能注入水的时间，请求才能通过限流器。令牌桶算法和漏桶算法很像一个硬币
的正反面，所以你可以参考令牌桶算法的实现来实现漏桶算法。

上面我们介绍了guava是如何实现令牌桶算法的，我们的实例代码是都guavaratelimiter的简化，
guava ratelimiter过站了标准的令牌桶算法，比如还能支持预热功能。对于按需加缓存来说，
预热后缓存能支持5W TPS的并发，但是在预热前5W TPS的并发直接就吧
缓存击垮了，所以如果需要给该缓存限流，限流器也需要支持预热功能，在初始阶段，显示的
流速R很小，但是动态增加的。预热功能的实现非常复杂，guava构建了一个积分函数来解决
这个问题，如果你感兴趣，可以继续研究。



39|案例分析 二 :高性能网络应用框架Netty

Netty是一个高性能网络应用框架，应用非常普遍，目前在java领域里，Netty基本上成为
网络程序的标配了。Netty框架功能丰富，也非常复杂，今天我们主要分析Netty框架中的线程
模型，而线程模型直接影响着网络程序的性能。

在介绍Netty的线程模型之前，我们首先需要把问题搞清楚，了解网络变长性能的瓶颈在那里
，然后再看Netty的线程模型是如何解决这个问题的。

网络变长性能的瓶颈
在33|Thread per message模式：最简单使用的分工方法 中，我们写过一个简单的
网络程序echo，采用的是阻塞式IO(BIO).BIO模型里，所有read操作和write操作
都会阻塞当前线程的，如果客户端已经和服务端建立了一个链接，而迟迟不发送数据，那么服务
端的read操作会一直阻塞，所以使用BIO模型，一般都会为每个socket分配一个独立
的线程，这样就不会因为线程阻塞在有一个socket上而影响对其他socket的读写。BIO的线程
模型如下图所示，每一个socket都对应一个独立的线程；为了避免频繁创建、消耗线程，
可以采用线程池，但是socket和线程之间的对应关系并不会变化。
block

BIO这种线程模型适用于socket连接不是很多的场景；但是现在的互联网场景，往往需要服务器
能够支撑十万甚至百万连接，而创建十万甚至上百万个线程先让并不现实，所以BIO线程
模型无法解决百万连接的问题。如果仔细观察，你会发现互联网场景中，虽然连接多，但是每个
连接上的请求并不频繁，所以线程大部分时间都在等待IO就绪。也就是说线程大部分时间
都阻塞在那里，这完全是浪费，如果我们能够解决这个问题，那就不需要这么多线程了。

顺着这个思路，我们可以将线程模型优化为下图这个样子，可以用一个线程来处理多个连接，
这样线程的利用率就上来了，同时所需的线程数量也跟着降下来了。这个思路很好，可是使用
BIO相关的API是无法实现的，这是为什么呢？因为BIO相关的socket读写操作都是阻塞式的，
而一旦调用了阻塞式API，在IO就续签，调用线程会一直阻塞，也就无法处理其他的
socket连接了。

好在java里还提高了非阻塞式NIO API，立勇非阻塞式API就能够实现一个线程处理多个
连接了。那具体如何实现呢？现在普遍都是采用Reactor模式，包括netty的实现。所以
，要想理解Netty的实现，接下来我们就需要先了解一下 Reactor 模式。

Reactor模式

下面是reactor模式的类结构图，其中handle值得是IO句柄，在java网络变长里，他本质
上就是一个网络连接。Event handler很容易理解，就是一个时间处理器，其中
handle event方法处理IO 事件，也就是每个event handler处理一个IO
handle；get handle 方法可以返回这个IO的handle。synchronous event
demultiplexer可以理解为操作系统提供的IO 多路复用API，例如POSIX标准里的select
以及linux里面的epoll


Reactor模式的核心自然是reactor这个类，其中register handler 和remove handler
这两个方法可以注册和删除一个事件处理器；handle events方式是核心，也是reactor
模式的发动机，这个方法的核心逻辑如下：首先通过同步事件多路选择器提供的select方法
监听网络时间，当有网络事件就绪后，就遍历事件处理器来处理该网络时间。由于网络时间是
源源不断的，所以在主程序中启动reactor模式，需要以whiletrue的方式调用
handle events方法。

Netty中的线程模型
netty的实现虽然参考了reactor模式，但是并没有完全照搬，netty中最核心的概念是事件
循环eventLoop,其实也就是Reactor模式中的Reactor，负责监听网络事件并调用事件
处理器进行处理。在4版本中的netty中，网络连接和EventLoop是稳定的多对1关系
而Eventloop和java线程是1对1关系，这里的稳定指的是关系一旦确定就不再发生变化。
也就是说一个网络连接只会对应唯一的一个eventloop，而一个eventLooop也只会对应
到一个java线程，所以一个网络连接只会对应到一个java线程。

一个网络连接对应到一个java线程上，有什么好处呢，最大的好处就是对于一个网络连接的
时间处理是单线程的，这样就避免了各种并发问题。

Netty中的线程模型可以参考下图，这个图和前面我们提到的理想的线程模型图非常相似，核心
目标都是用一个线程处理多个网络连接。

Netty中还有一个核心概念eventloopgroup顾名思义，一个eventLoopGroup由一组
Eventloop组成。实际使用中，一般都会创建两个EventLoopGroup一个称为
bossGroup，一个称为workergroup。为什么会有两个eventLoopGroup呢？
这个和socket处理网络请求的机制有关，socket处理TCP网络连接请求，是在一个独立的
socket中，每当一个TCP连接成功建立，都会创建一个新的socket，之后对TCP连接的
读写都是新创建处理的socket完成的。也就是说处理TCP连接请求和读写请求是通过两个
不同的socket完成的。上面我们就讨论网络请求的时候，为了简化模型，只是讨论了读写请求
，而没有讨论连接请求。

在netty中，bossGroup就用来处理连接请求的，而workergroup是用来处理读写请求的。
bossgroup处理完连接请求后，会将这个连接提交给workergroup来处理，
workergroup里面有多个eventloop，那新的连接会交给那个eventloop来处理呢？这就
需要一个负载均衡算法，netty中国模前使用的是轮训算法。

下面我们用netty重新实现以下echo程序的服务端，近距离感受以下netty。


用netty实现echo程序服务端
下面的实例代码基于netty实现echo程序服务端：首先创建了一个事件处理器等同于
reactor模式中的事件处理器，然后创建了bossgrou和workerGroup，再之后创建并
初始化了serverbootstrap，代码还是很简单的，不过有两个地方需要注意一下。

第一个，如果nettyboosgroup只监听一个端口，那bossGroup只需要一个eventLoop就可以了
多了纯属浪费。

第二个，默认情况下，netty会创建2cpu核数个eventloop，由于网络谅解与
eventloop有稳定的关系，所以事件处理器在处理网络事件的是是不能有阻塞操作的，否则
很容易导致请求大面积超时。如果实在无法避免使用阻塞操作，那可以通过线程池来异步处理。

总结

Netty是一个优秀的网络变长框架，性能非常好，为了实现高性能的目标，nett做了很多
优化，假如优化了bytebuffer支持零拷贝等等，和并发变长相关的就是它的线程模型了。
netty的线程模型设计得很精巧，每个网络连接都关联到了一个线程上，这样做的好处是：对于
一个网络连接，读写操作都是单线程执行的，从而避免了并发程序的各种问题。

你要想深入理解netty的线程模型，还需要对网络相关知识有一定的理解，关于javaio的演进过程
，你可以参考
http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf
至于TCP IP网络变长的只是你可以参考韩国尹盛郁写的TCPIP网络编程。





40|案例分析三：高性能队列 disruptor 破坏者

我们在20并发容器：都有哪些坑需要我们填 介绍过javasdk提供了两个有界
队列：arrayBlockingQueue和linkedBlockingQueue,他们都是基于ReentrantLock实现
的，在高并发场景下，锁的效率并不高，那有么有更好的替代品呢？有，今天我们就介绍一种
性能更高的有界队列：disruptor

disruptor 是一款高性能的有界内存队列，目前应用非常广泛，log4j2,spring
messaging,hbase storm都用到了 disruptor,那disruptor的性能为什么这么高呢？
disruptor项目团队曾经写过一篇论文，详细解释了其原因，可以总结为如下：

1.内存分配更加合理，使用RingBuffer数据结构，数据元素在初始化时一次性全部创建，提升
缓存命中率；对象循环利用，避免频繁GC。

2.能够避免伪共享，提升缓存利用率。

3.采用无锁算法，避免频繁加锁、解锁的性能消耗。

4.支持批量消费，消费者可以无锁方式消费多个消息。

其中，前三点设计到的只是比较多，所以今天咱们重点讲解前三点，不过在详细介绍这些知识
之前，我们先来聊聊 disruptor如何使用，好让你先对 disruptor有个感官的认识。

下面的代码出自官方示例，我略做了一些修改，相较而言，disruptor的使用比javasdk提供
blockingqueue要复杂一些，但是总体思路还是一直的，其大致情况如下：

在disruptor中，生产者生产的对象称为event，使用
disruptor必须自定义event，例如示例代码的自定义event是longevent

构建disruptor对象除了要指定队列大小外，还需要传入一个eventfactory，示例代码中
传入的是longEvent new;

消费disruptor中的event需要通过handle EventsWith方法注册一个事件处理器，发布
event则需要通过publishEvent方法。

RingBuffer如何提升性能

javasdk中arrayBlockingQueue使用数组座位底层的数据存储，而disruptor是使用
RingBuffer作为数据存储。RingBuffer本质上也是数组，所说义仅仅将数据存储从数组换成
RingBuffer并不能提升性能，但是disruptor在RingBuffer的基础上还做了很多优化，其中
一项优化就是和内存分配有关的。

在介绍这项优化之前，你需要先了解一下程序的局部性原理。简答来讲，程序的局部性原理指的
是在一段时间内程序的执行会限定在一个局部范围内。这里的局部性可以从两个方面来
理解，一个是时间局部性，另一个是空间局部性。时间局部性指的是程序中的某条指令一旦被
执行，不就之后这条指令很可能再次被执行；如果某条数据被访问，不就之后这套数据很可能
再次被访问。而空间局部性是指某块内存一旦被访问，不久之后这块内存附近的内存也很有可能
被访问。

CPU的缓存就利用了程序的局部性原理：CPU从内存中加载数据X时，会将数据X缓存在高速
缓存cache中，实际上CPU环境X的同时还缓存了X周围的数据，因为根据程序具备
局部性原理，X周围的数据也很有可能被访问。从另外一个角度来看，如果程序能够很好地体现
出局部性原理，也就能更好地地利用CPU缓存，从而提升程序的性能。disruptor在设计
RingBuffer的时候就充分考虑了这个问题，下面我们就对比着arrayblockingqueue来分析
一下。

首先是arrayBlockingqueue。生产者线程向ArrayBlockingqueue增加一个元素，每次增加
元素E之前，都需要创建一个对象E，如下图所示，arrayBlockingQueue内部有6个元素，
这6个元素都是由生产者线程创建的，由于创建这些元素的时间基本上是离散的，所以这些
元素的内存地址大概率也不是连续的。

下面我们再看看disruptor是如何处理的。disruptor内部的ringbuffer也是用数据实现的
，但是这个数组中的所有元素在初始化时是一次性全部创建的，所以这些元素的内存地址大概率
是连续的，相关的代码如下所示。

for(int i=0;i<bufferSize;i++){
  entries[BUFFER_PAD+I]=eventFactory.newInstance();
}

Disruptor内部RingBuffer 的结构可以简化成下图，那问题来了，数组中所有元素内存地址
连续能提升性能吗？能！为什么呢？因为消费者线程在消费的时候，是遵循控件局部性原理的
，消费完第一个元素，很快就会消费第二个元素；当消费第一个元素E1的时候，CPU会
把内存中E1后面的数据也加载进cache，如果E1和E2在内存中的地址是连续的，那么E2
也就会被加载进cache中，然后当消费第二个元素的时候，由于E2已经在cache中了，所以
就不需要从内存中加载了，这样就能大大提升性能。

除此之外，在disruptor中，生产者线程通过publishEvent发布event的时候，并不是创建一个新的
Event，而是通过eventset方法修改event，也就是说ringbuffer创建的
event是可以循环利用的，这样还能避免频繁创建删除event导致的频繁GC问题。

如何避免伪共享
高效利用cache，能够大大提升性能，所以要努力构建能够高效利用cache的内存结构。而
从另外一个角度看，努力避免不能高效利用cache的孽畜结构也同样重要。

有一种叫做伪共享 false sharing 的内存布局就会使cache失效，那什么是伪共享
呢？

伪共享和CPU内部的cache有关，cache内部是按照缓存行 cache line管理的，缓存
行的大小通常是64个字节；CPU从内存中加载数据X，会同时加载X后面64-size个
字节的数据。下面的实例代码出自javasdk的arrayblockingqueue，其内部维护了四个成员
变量，分别是队列数组items、出队索引takeindex，如对索引putindex以及队列中的元素
总数count

final Object[] items;

int takeIndex;

int putIndex;

int count;

当CPU从内存中加载takeindex的时候，会同时将putIndex以及count都加载进cache
下图是某个时刻CPU中cache的状况，为了简化，缓存行中我们仅列出了takeindex和
putindex。

假设线程A运行在CPU-1上，执行入队操作，入队操作会修改putindex，而修改putIndex
会导致其所在的所有核上的缓存行均失效；此时假设运行在CPU2上的线程执行出兑操作，
出队操作需要读取takeindex，由于takeIndex所在的缓存行已经失效，所以CPU2必须从
内存中重新读取。入队操作本不会修改takeIndex，但是由于takeIndex和putIndex共享的
是一个缓存行，就导致出队操作不能很好利用Cache，这其实就是伪共享。简单来讲，伪共享
指的是由于共享缓存行导致缓存无效的场景。

ArrayBlockingQueue的入队和出队操作是用锁来保证互斥的，所以入队和出队不会同时发生
。如果允许入队和出队同时发生，那就会导致线程A和线程B争用同一个缓存行，这样也会
导致性能问题。所以为了更好地利用缓存，我们必须避免伪共享，那如何避免呢？

方案很简单，每个变量独占一个缓存行，不共享缓存行就尅了，具体技术是缓存行填充。比如
想让takeIndex独占一个缓存行，可以在takeIndex的前后各填充56个字节，这样就一定
能保证takeIndex独占一个缓存行。下面的实例代码出自disruptor sequence对象中的
vaule属性就能避免伪共享，因为这个属性前后都填充了56个字节。disruptor中很多对象
，例如RingBuffer，RingBuffer内部的数组都用到了这种填充技术来避免伪共享。


disruptor 中的无锁算法

ArrayBlockingQueue是利用管程实现的，中规中矩，生产、消费操作都需要加锁，实现起来
简单，但是性能并不十分理想。disruptor采用的是无锁算法，很复杂，但是核心无非是生产
和消费两个操作。disruptor中最复杂的是入队操作，所以我们重点来看看入队操作是如何实现的。

对于入队操作，最关键的是要求是不能覆盖没有消费的元素；对于出队操作，最关键的要求是不能
读取没有写入的元素，所以disruptor中也一定会维护类似出队索引和入队索引这样两个关键
变量。disruptor中的RingBuffer维护了入队索引，但是并没有维护出队索引，这是因为
在disruptor中多个消费者可以同时消费，每个消费者都会有一个出队索引，所以
RingBuffer的出队索引是所有消费者里面最小的那一个。

下面是disruptor生产者入队操作的核心代码，看上去很复杂，其实逻辑很简单：如果没有足够
的空余位置，就出让CPU使用权，然后重新计算；反之则用CAS设置入队索引。


总结
disruptor在优化并发性能方面科委是做到了极致，优化的思想答题是两个方面，一个是利用
无所算法避免所的争用，另外一个则是将硬件CPU的性能发挥到极致。尤其是后者，在
java领域基本上属于经典之作了。

发挥硬件的能力一般是C这种面相赢家你的语言长干的事儿，C语言领域经常通过调整内存布局
优化内存占用，而Java领域则用的很少，原因是在于java可以智能地优化内存布局，内存布局
对java程序的透明的。这种智能的优化大部分常军是很友好的，是单如果你想通过填充方式
避免伪共享就必须绕过这种优化，关于这方面disruptor提供了经典的实现，你可以参考。

由于伪共享问题如何重要，所以java也开始重视它了，比如java9中，提供了避免伪共享的
注解:sun.misc.Contended,通过这个注解就能轻松避免伪共享。需要设置jvm参数
-XX:-RestrictContended 不过避免伪共享是以牺牲内存为代价的，所以具有使用的时候还是
需要仔细斟酌。


41|案例分析四：高性能数据库连接池HiKariCP

实际工作中，我们总会难免和数据库打交道；只要和数据库打交道，就免不了使用数据库连接池
。业界知名的数据库连接池有不少，例如c2p0 DBCP tomcatJDBC Connection Pool
druid ,不过最近最火的是hikariCp

HiKariCP号称是业界跑得最快的数据库连接池，这两年发展的顺风顺水，尤其是
SpringBoot2.0将其作为默认数据库连接池后，江湖一哥的地位已是毋庸置疑了。那它为什么
那么快呢？今天咱们就重点聊聊这个话题。

绅士数据库连接池

在详细分析hikaricp高性能之前，我们有必要先简单介绍以下什么是数据库连接池。本质
上，数据库连接池和线程池一样，都属于池化资源，作用都是避免重量级资源的频繁创建和销毁，
对于数据库连接池来说，也就是避免数据库连接频繁创建和销毁。如下图所示，服务端会
在运行期持有一点数量的数据库连接，当需要执行SQL时，并不是直接创建一个数据库连接，
而是从连接池中获取一个；当SQL执行完，也并不是将数据库连接真的关掉，而是将其
归还到连接池中。

在实际工作中，我们都是使用各种持久化框架来完成数据库的增删改查，基本上不会直接和数据库
连接池打交道，为了能让你更好地理解数据库连接池的工作原理，下面的实例代码并没有
使用任何框架，而是原生的使用hikariCP。执行数据库操作基本上是一系列规范化的步骤：

1.通过数据源获取一个数据库连接；
2.创建statement
3.执行SQL
4.通过ResultSet获取SQL执行结果；
5.释放Resultset
6.释放statment；
7.释放数据库连接

下面的实例代码，通过ds.getConnection获取一个数据库连接时，其实是向数据库连接池
申请一个数据库连接，而不是创建一个新的数据库连接。同样，通过conn.close释放
一个数据库连接时，也不是直接将连接关闭，而是将连接归还给数据库连接池。

HikariCP
https://github.com/brettwooldridge/HikariCP/wiki/Down-the-Rabbit-Hole


HiKariCP官方网站解释了其性能之所以如此之高的秘密。微观上HikariCp程序编译出的字节码
执行效率更高，站在字节码的角度去优化java代码，hikaricp的作者对性能的执着可见一斑，
不过以涵的是他并没有详细解释都做了那些优化。而宏观上主要是和两个数据结构有关，
一个是fastlist，另一个是concurrentBag。下面我们来看看他们是如何提升hikaricp的
性能的。

FastList解决了那些性能问题
按照规范步骤，执行完数据库操作之后，需要依次关闭ResultSet,statement
connectiuon,但是总有粗心的同学只是关闭了Connection，而忘了关闭resultSet和
statment。为了解决这种问题，最好的办法是当关闭connection时，能够自动关闭
statment。为了达到这个目标，connection就需要跟踪创建的statment最简单的办法
就是将创建的statment报错在数组arraylist里，这样当关闭connection的时候，就可以
依次将数组总的所有statment关闭。

HiKariCP觉得用arraylist还是太慢，当通过conn.createStatment创建一个
staement时，需要调用arrayList的add方法加入到arraylist中，这个是没有问题的；
但是当通过stmt.close关闭stament的时候，需要调用arraylist的remove方法
来将其从arrraylist中删除，这里是有优化余地的。

假设一个connection依次创建6个statment，分别是是s1 s6按照
正常的编码习惯，关闭statment的顺序一般是逆序的，关闭的顺序是s6 s1
而arrayList的remove方法是顺序遍历查询，逆序删除而顺序查找，这样
的查找效率就太慢了。如何优化呢？很简单，优化陈给你续查询就可以了。

Hikaricp中的fastlist相对于arraylist的一个优化就是将remove
方法的查找顺序变成了逆序查找。除此之外，fastlist还有另一个优化点，是get
方法没有对index参数进行越界检查，hikaricp能保证不会越界，所以不用每次都
进行越界检查。

整体来看，fastlist的优化点还是很简单的。下面我们再来聊聊HikariCP中的另外一个数据
结构concurrentBag，看看它有事如何提升性能的。

ConcurrentBag解决了那些性能问题

如果让我们自己来实现一个数据库连接池，最简单的办法就是用两个阻塞队列来实现，一个用于
报错空闲数据库连接的队列 idle，另一个用于报错忙碌数据库连接的队列busy获取连接
时将空闲的数据库连接从idel队列移动到busy队列，而关闭连接时讲数据库连接从busy移动
到idle。这种方案将并发问题委托给了阻塞队列，实现简单，但是性能并不是很理想。因为
javasdk中的阻塞队列是用锁实现的，而高并发常军下锁的争用对性能呢影响很大。

HikariCP并没有使用javasdk中的阻塞队列，而是自己实现了一个叫做concurrentbag的
并发容器。concurrentbag的设计最初源自于C它的一个核心设计是使用threadlocal避免
部分并发问题，不过hikaricp中的concurrentbag并没有完全参考C的实现，下面我们
来看看他是如何实现的。

ConcurrentBag中最关键的属性有四个，分别是：用于存储所有的数据库连接的共享队列
sharedlist，线程本地存储threadlist、等待数据库连接的线程数waiters以及分配数据库连接
的工具handoffqueue。其中，韩豆腐坊Queue用的是javaSDK提供的
SynchronousQueue,SynchronousQueue主要用于线程之间传递数据。



// 用于存储所有的数据库连接
CopyOnWriteArrayList<T> sharedList;
// 线程本地存储中的数据库连接
ThreadLocal<List<Object>> threadList;
// 等待数据库连接的线程数
AtomicInteger waiters;
// 分配数据库连接的工具
SynchronousQueue<T> handoffQueue;


当线程池创建了一个数据库连接时，通过调用concurrentBag的add方法加入到
concurrentBag中，下面是add方法的具体实现，逻辑很简单，就是将这个连接加入到共享
队列sharedlist中，如果此时有现成在等待数据库连接，那么就通过handoffQueue讲这个
连接分配给等待的线程。

通过concurrentBag提供的borrow方法，可以获取一个空闲的数据库连接，borrow的
主要逻辑是：

1.首先查看线程本地存储是否有空闲连接，如果有，则返回一个空闲的连接；

2.如果线程本地存储中午空闲连接，则从共享队列中获取。

3.如果共享队列中也没有空闲的连接，则请求线程要求等待。

需要注意的是，线程本地存储中的连接是可以被其他线程窃取的，所以需要用CAS方法防止
重复分配。在共享队列中获取空闲连接，也采用了CAS方法防止重复分配。


T borrow(long timeout, final TimeUnit timeUnit){
  // 先查看线程本地存储是否有空闲连接
  final List<Object> list = threadList.get();
  for (int i = list.size() - 1; i >= 0; i--) {
    final Object entry = list.remove(i);
    final T bagEntry = weakThreadLocals 
      ? ((WeakReference<T>) entry).get() 
      : (T) entry;
    // 线程本地存储中的连接也可以被窃取，
    // 所以需要用 CAS 方法防止重复分配
    if (bagEntry != null 
      && bagEntry.compareAndSet(STATE_NOT_IN_USE, STATE_IN_USE)) {
      return bagEntry;
    }
  }

  // 线程本地存储中无空闲连接，则从共享队列中获取
  final int waiting = waiters.incrementAndGet();
  try {
    for (T bagEntry : sharedList) {
      // 如果共享队列中有空闲连接，则返回
      if (bagEntry.compareAndSet(STATE_NOT_IN_USE, STATE_IN_USE)) {
        return bagEntry;
      }
    }
    // 共享队列中没有连接，则需要等待
    timeout = timeUnit.toNanos(timeout);
    do {
      final long start = currentTime();
      final T bagEntry = handoffQueue.poll(timeout, NANOSECONDS);
      if (bagEntry == null 
        || bagEntry.compareAndSet(STATE_NOT_IN_USE, STATE_IN_USE)) {
          return bagEntry;
      }
      // 重新计算等待时间
      timeout -= elapsedNanos(start);
    } while (timeout > 10_000);
    // 超时没有获取到连接，返回 null
    return null;
  } finally {
    waiters.decrementAndGet();
  }
}

释放连接需要调用ConcurrentBag提供的requite方法，该方法的逻辑很简单，首先将数
据库连接状态更改为state not in use 之后查看是否存在等待线程，如果有，则分配给
等待线程；如果没有，则将该数据库连接报错到线程本地存储里。


总结
HIkariCP 中的fastList和ConcurrentBag这两个数据结构使用的非常巧妙，虽然实现起来并不
复杂，但是对于性能的提升非常明显，根本原因在于这两个数据结构适用于数据库连接池这个
特定的常军。fastlist适用于逆序删除场景；而concurrentbag通过threadlocal做一次
预分配，避免直接竞争共享资源，非常适合池化资源的分配。

在实际工作中，我们遇到的并发问题千差万别，这时选择合适的并发数据结构就非常重要了。
当然能选对的前提是都特定常军的并发性有深入的了解，只有了解到吴伟的性能消耗在那里，
才能对症下药。



































