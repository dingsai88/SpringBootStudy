

38 | 案例分析（一）：高性能限流器Guava RateLimiter

从今天开始，我们就进入案例分析模块了。这个模块我们将分析四个经典的开源框架，看看它们
是如何处理并发问题的，通过这四个案例的学习，相信你会对如何解决并发问题有个更深入
的认识。

首先 guava  RateLimiter 是如何解决高并发场景下的限流问题的。guava是
google开源的类库，提供了一个工具类RateLimiter .我们先来看看 RateLimiter的使用
，让你对限流有个感官上的印象。假设我们有一个线程池，它每秒只能处理两个任务，如果提交
的任务过快，可能导致系统不稳定，这个时候就需要用到限流。

在下面的实例代码中，我们创建了一个流速为2个请求每秒的限流器，这个里的流速该怎么理解呢
直观地看，2个请求每秒值得是每秒最多允许2个请求通过限流器。其实在guava中
流速还有更深层的意思：是一种匀速的概念，2个请求每秒等价于1个请求500毫秒。

经典限流算法：令牌桶算法
guava的限流器使用上还是很简单的,那它是如何实现的呢？guava采用的是令牌桶算法，
其核心是要想通过限流器，必须拿到令牌。也就是说，只要我们能够限制发放令牌的速率，那么
就能控制流速了。令牌桶算法的详细描述如下：
1.令牌以固定的速率添加到令牌桶中，假设限流的速率是R/秒，则令牌每1/r秒会添加一个；

2.假设令牌桶的容量是b，如果令牌桶已满，则新的令牌会被丢弃；

3.请求能够通过限流器的前提是令牌桶中有令牌。

这个算法中，限流的速率r还是比较容易理解的，但令牌桶的容量b该怎么理解呢？b其实是
burst的简写，意义是限流器允许的最大突发流量。比如b=10,而且令牌桶中的令牌已满，
此时限流器允许10个请求同事通过限流器，当然只是突发流量而已，这10个请求会带走10
个令牌，所以后续的流量只能按照速率r通过限流器。

令牌桶这个算法，如何用java实现呢？很可能你的直接会告诉你生产者-消费者模式：一个
生产者线程定时向阻塞队列中添加令牌，而试图通过限流器的线程则作为消费者线程，只有从
阻塞队列中获取到令牌，才运行通过限流器。

这个算法看上去非常完美，而且实现起来非常简单，如果并发量不打，这个实现并没有什么问题
。可实际情况却是使用限流的常军大部分都是高并发常军，而且系统压力已经临近极限了，
此时这个实现就有问题了。问题就出在定时器上，在高并发场景下，当系统压力已经临近极限的时候
，定时器的精度误差会非常大，同时定时器本身会创建调度线程，也会对系统的性能产生
影响。

那还有什么好的实现方式呢？当然有，guava的实现就没有使用定时器，下面我们就来看看它是
如何实现的。


Guava如何实现令牌桶算法
guava实现令牌桶算法，用了一个很简单的办法，其关键是记录并动态计算下一令牌发放的时间
。下面我们以一个最简单的常军来介绍该算法的执行过程。假设令牌桶的容量为B=1，限流
速率R=1个请求秒 如下图所示，如果当前令牌桶中没有令牌，下一个令牌的发放时间是在
第三秒，而在第二秒的时候有一个线程T1请求令牌，此时该如何处理呢？

对于这个请求令牌的线程而言，很先让需要等待1秒，因为1秒以后它就能拿到
令牌了。此时需要注意的是，下一个令牌发放的时间也要增加1秒，为什么呢？因为第三秒
发放的令牌已经被线程T1预占了。处理之后如下图所示。

假设T1在预占了第三秒的令牌之后，马上又有一个线程T2请求令牌，如下图所示。

很显然，由于下一个令牌产生的时间是第四秒，所以线程T2要等待两秒的时间，才能获取到
令牌，同时由于T2预占了第四秒的令牌，所以下一令牌产生还要增加1秒，完全处理之后，
如下图所示。

上面线程T1 T2都是在下一令牌产生时间之前请求令牌，如果线程在下一令牌产生时间之后
请求令牌会如何呢？假设在线程T1请求令牌之后的5秒，也就是第七秒，线程T3请求令牌
如下图所示。

由于在第五秒已经产生了一个令牌，所以此时线程T3可以直接拿到令牌，而无需等待。在第
7秒，实际上限流器能够产生3个令牌，第567秒各产生一个令牌。由于我们假设令牌
桶的容量是1，所以第67秒产生的令牌就丢弃了，其实等价地你可以认为是保留的第四秒的令牌
，丢弃的第56秒的令牌，也就是说第7秒的令牌呗线程T3占有了，于是下一令牌
的的产生时间应该是第8秒，如下图所示。



通过上面简要地分析，你会发现，我们只需要记录一个下一令牌产生的时间，并动态更新它，
就能够轻松完成限流功能。我们可以将上面的这个算法代码化，实例代码如下所示，依然假设
令牌桶的容量是1.关键 reserve 方法，这个方法会为请求令牌的线程预分配令牌，同时
返回该线程能够获取令牌的时间。其实现逻辑就是上面提到的：如果线程请求令牌的时间在下一
令牌产生时间之后，那么该线程立刻就能够获取令牌；反之，如果请求时间在下一令牌产生
时间之前，那么该线程是在下一令牌产生的时间获取令牌。由于此时下一令牌已经被该线程预先占有
，所以下一令牌产生的时间需要加上1秒。
reserve
acquire

SimpleLimiter

如果令牌桶的容量大于1，又该如何处理呢？按照令牌桶算法，令牌要首先从令牌桶中出，所以
我们需要按需计算令牌桶中的数量，当有线程请求令牌时，先从令牌桶中出。具体的代码实现
如下所示。我们增加了一个 resync 方法，在这个方法中，如果线程请求令牌的时间在下
一令牌产生时间之后，会重新计算令牌桶中的领牌数，新产生的令牌的计算公式是：now-next/interval
你可对照上面的示意图来理解。 reserve 方法中，则增加了先从令牌桶中出
令牌的逻辑，不过需要注意的是，如果令牌是从令牌桶中出的，那么next就无需增加一个
interval了。




总结

经典的限流算法有两个，一个是令牌桶算法 token bucket ，另一个是漏桶算法 leaky bucket.
令牌桶算法是定时向令牌桶发送令牌，请求能够从令牌桶中拿到令牌，然后才能
通过限流器；而漏铜算法里，请求就像水一样注入漏桶，漏桶会按照一定的速率自动将水漏掉，
只有漏桶里还能注入水的时间，请求才能通过限流器。令牌桶算法和漏桶算法很像一个硬币
的正反面，所以你可以参考令牌桶算法的实现来实现漏桶算法。

上面我们介绍了guava是如何实现令牌桶算法的，我们的实例代码是都guavaratelimiter的简化，
guava ratelimiter过站了标准的令牌桶算法，比如还能支持预热功能。对于按需加缓存来说，
预热后缓存能支持5W TPS的并发，但是在预热前5W TPS的并发直接就吧
缓存击垮了，所以如果需要给该缓存限流，限流器也需要支持预热功能，在初始阶段，显示的
流速R很小，但是动态增加的。预热功能的实现非常复杂，guava构建了一个积分函数来解决
这个问题，如果你感兴趣，可以继续研究。



39|案例分析 二 :高性能网络应用框架Netty

Netty是一个高性能网络应用框架，应用非常普遍，目前在java领域里，Netty基本上成为
网络程序的标配了。Netty框架功能丰富，也非常复杂，今天我们主要分析Netty框架中的线程
模型，而线程模型直接影响着网络程序的性能。

在介绍Netty的线程模型之前，我们首先需要把问题搞清楚，了解网络变长性能的瓶颈在那里
，然后再看Netty的线程模型是如何解决这个问题的。

网络变长性能的瓶颈
在33|Thread per message模式：最简单使用的分工方法 中，我们写过一个简单的
网络程序echo，采用的是阻塞式IO(BIO).BIO模型里，所有read操作和write操作
都会阻塞当前线程的，如果客户端已经和服务端建立了一个链接，而迟迟不发送数据，那么服务
端的read操作会一直阻塞，所以使用BIO模型，一般都会为每个socket分配一个独立
的线程，这样就不会因为线程阻塞在有一个socket上而影响对其他socket的读写。BIO的线程
模型如下图所示，每一个socket都对应一个独立的线程；为了避免频繁创建、消耗线程，
可以采用线程池，但是socket和线程之间的对应关系并不会变化。
block

BIO这种线程模型适用于socket连接不是很多的场景；但是现在的互联网场景，往往需要服务器
能够支撑十万甚至百万连接，而创建十万甚至上百万个线程先让并不现实，所以BIO线程
模型无法解决百万连接的问题。如果仔细观察，你会发现互联网场景中，虽然连接多，但是每个
连接上的请求并不频繁，所以线程大部分时间都在等待IO就绪。也就是说线程大部分时间
都阻塞在那里，这完全是浪费，如果我们能够解决这个问题，那就不需要这么多线程了。

顺着这个思路，我们可以将线程模型优化为下图这个样子，可以用一个线程来处理多个连接，
这样线程的利用率就上来了，同时所需的线程数量也跟着降下来了。这个思路很好，可是使用
BIO相关的API是无法实现的，这是为什么呢？因为BIO相关的socket读写操作都是阻塞式的，
而一旦调用了阻塞式API，在IO就续签，调用线程会一直阻塞，也就无法处理其他的
socket连接了。

好在java里还提高了非阻塞式NIO API，立勇非阻塞式API就能够实现一个线程处理多个
连接了。那具体如何实现呢？现在普遍都是采用Reactor模式，包括netty的实现。所以
，要想理解Netty的实现，接下来我们就需要先了解一下 Reactor 模式。

Reactor模式

下面是reactor模式的类结构图，其中handle值得是IO句柄，在java网络变长里，他本质
上就是一个网络连接。Event handler很容易理解，就是一个时间处理器，其中
handle event方法处理IO 事件，也就是每个event handler处理一个IO
handle；get handle 方法可以返回这个IO的handle。synchronous event
demultiplexer可以理解为操作系统提供的IO 多路复用API，例如POSIX标准里的select
以及linux里面的epoll


Reactor模式的核心自然是reactor这个类，其中register handler 和remove handler
这两个方法可以注册和删除一个事件处理器；handle events方式是核心，也是reactor
模式的发动机，这个方法的核心逻辑如下：首先通过同步事件多路选择器提供的select方法
监听网络时间，当有网络事件就绪后，就遍历事件处理器来处理该网络时间。由于网络时间是
源源不断的，所以在主程序中启动reactor模式，需要以whiletrue的方式调用
handle events方法。

Netty中的线程模型
netty的实现虽然参考了reactor模式，但是并没有完全照搬，netty中最核心的概念是事件
循环eventLoop,其实也就是Reactor模式中的Reactor，负责监听网络事件并调用事件
处理器进行处理。在4版本中的netty中，网络连接和EventLoop是稳定的多对1关系
而Eventloop和java线程是1对1关系，这里的稳定指的是关系一旦确定就不再发生变化。
也就是说一个网络连接只会对应唯一的一个eventloop，而一个eventLooop也只会对应
到一个java线程，所以一个网络连接只会对应到一个java线程。

一个网络连接对应到一个java线程上，有什么好处呢，最大的好处就是对于一个网络连接的
时间处理是单线程的，这样就避免了各种并发问题。

Netty中的线程模型可以参考下图，这个图和前面我们提到的理想的线程模型图非常相似，核心
目标都是用一个线程处理多个网络连接。

Netty中还有一个核心概念eventloopgroup顾名思义，一个eventLoopGroup由一组
Eventloop组成。实际使用中，一般都会创建两个EventLoopGroup一个称为
bossGroup，一个称为workergroup。为什么会有两个eventLoopGroup呢？
这个和socket处理网络请求的机制有关，socket处理TCP网络连接请求，是在一个独立的
socket中，每当一个TCP连接成功建立，都会创建一个新的socket，之后对TCP连接的
读写都是新创建处理的socket完成的。也就是说处理TCP连接请求和读写请求是通过两个
不同的socket完成的。上面我们就讨论网络请求的时候，为了简化模型，只是讨论了读写请求
，而没有讨论连接请求。

在netty中，bossGroup就用来处理连接请求的，而workergroup是用来处理读写请求的。
bossgroup处理完连接请求后，会将这个连接提交给workergroup来处理，
workergroup里面有多个eventloop，那新的连接会交给那个eventloop来处理呢？这就
需要一个负载均衡算法，netty中国模前使用的是轮训算法。

下面我们用netty重新实现以下echo程序的服务端，近距离感受以下netty。


用netty实现echo程序服务端
下面的实例代码基于netty实现echo程序服务端：首先创建了一个事件处理器等同于
reactor模式中的事件处理器，然后创建了bossgrou和workerGroup，再之后创建并
初始化了serverbootstrap，代码还是很简单的，不过有两个地方需要注意一下。

第一个，如果nettyboosgroup只监听一个端口，那bossGroup只需要一个eventLoop就可以了
多了纯属浪费。

第二个，默认情况下，netty会创建2cpu核数个eventloop，由于网络谅解与
eventloop有稳定的关系，所以事件处理器在处理网络事件的是是不能有阻塞操作的，否则
很容易导致请求大面积超时。如果实在无法避免使用阻塞操作，那可以通过线程池来异步处理。

总结

Netty是一个优秀的网络变长框架，性能非常好，为了实现高性能的目标，nett做了很多
优化，假如优化了bytebuffer支持零拷贝等等，和并发变长相关的就是它的线程模型了。
netty的线程模型设计得很精巧，每个网络连接都关联到了一个线程上，这样做的好处是：对于
一个网络连接，读写操作都是单线程执行的，从而避免了并发程序的各种问题。

你要想深入理解netty的线程模型，还需要对网络相关知识有一定的理解，关于javaio的演进过程
，你可以参考
http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf
至于TCP IP网络变长的只是你可以参考韩国尹盛郁写的TCPIP网络编程。
























