





I.架构定义

1.框架和架构的区别还是比较明显的，框架关注的是“规范”，架构关注的是“结构”



架构定义:软件架构指软件系统的顶层结构

架构目的:架构设计的主要目的是为了解决软件系统复杂度带来的问题


I.架构复杂度来源-高性能

架构复杂度来源
1.高性能

1.1单台计算机内部为了高性能带来的复杂度
Nginx多进程多线程，Redis单进程，Memcache采用的是多线程，这些系统都实现了高性能，但内部实现差异却很大。






1.2多台计算机集群为了高性能带来的复杂度
1.2.1任务分配

1.任务分配器 F5 LVS  nginx haproxy(考虑性能、成本、可维护性、可用性)


2.任务连接和交互  (连接建立、连接检测、连接中断后如何处理等。)

3.任务分配器需要增加分配算法 (顺序 随机 最小活跃 hash)LVS(顺序 随机 最小活跃 )



1.2.2 任务分解
简单的系统更加容易做到高性能(模块化用户模块)

可以针对单个任务进行扩展 只优化用户模块分库分表








I.架构复杂度来源-高可用 无中断

本质上都是通过“冗余”来实现高可用

 和高性能加机器对比:
高性能增加机器目的在于“扩展”处理性能；
高可用增加机器目的在于“冗余”处理单元。



1.计算高可用
增加一个任务分配器
任务分配器和真正的业务服务器之间有连接和交互
任务分配器需要增加分配算法。例如，常见的双机算法有主备、主主，主备方案又可以细分为冷备、温备、热备。

ZooKeeper一主多备，Memcached全主0备。



2.存储高可用:规避数据不一致对业务造成的影响
将数据从一台机器搬到到另一台机器，需要经过线路进行传输

存储高可用的难点不在于如何备份数据，而在于如何减少或者规避数据不一致对业务造成的影响。

CAP定理，从理论上论证了存储高可用的复杂度。也就是说，存储高可用不可能同时满足“一致性、可用性、分区容错性”，最多满足其中两个，这就要求我们在做架构设计时结合业务进行取舍。

CAP定理:一致性、可用性、分区容错性最多满足其中两个


3.高可用状态决策
通过冗余来实现的高可用系统，状态决策本质上就不可能做到完全正确。
3.1. 独裁式

3.2 协商式  主备

3. 民主式 ，ZooKeeper 集群选举






I.架构复杂度来源-可扩展性

正确预测变化、完美封装变化

不断有新的需求需要实现

1.应对变化
第一种应对变化的常见方案是将“变化”封装在一个“变化层”，将不变的部分封装在一个独立的“稳定层”。

1. 系统需要拆分出变化层和稳定层

2. 需要设计变化层和稳定层之间的接口


2.提炼出抽象层和实现层 设计模式
第二种常见的应对变化的方案是提炼出一个“抽象层”和一个“实现层”





I.架构复杂度来源-低成本、安全、规模

低成本


安全
1. 功能安全
XSS 攻击、CSRF 攻击、SQL 注入、Windows 漏洞、密码破解

2. 架构安全 DDoS 攻击



3.规模
系统往往功能特别多，逻辑分支特别多。
1. 功能越来越多，导致系统复杂度指数级上升
2. 数据越来越多，系统复杂度发生质变









I.架构三原则

合适原则:合适优于业界领先
1. 将军难打无兵之仗:没那么多人，却想干那么多活，是失败的第一个主要原因。
2. 罗马不是一天建成的:没有那么多积累，却想一步登天，是失败的第二个主要原因。
3. 冰山下面才是关键:没有那么卓越的业务场景，却幻想灵光一闪成为天才，是失败的第三个主要原因。




简单原则:简单优于复杂

1. 结构的复杂性
组件越多，就越有可能其中某个组件出现故；某个组件改动，会影响关联的所有组件；定位一个复杂系统中的问题总是比简单系统更加困难

2. 逻辑的复杂性
产品逻辑复杂




演化原则:演化优于一步到位
1.首先，设计出来的架构要满足当时的业务需要。
2.不断迭代，保留优秀的设计，修复有缺陷的设计，使得架构逐渐完善。
3.业务变化要扩展、重构，甚至重写；有价值的经验、教训、逻辑、设计却可以在新架构中延续。

 



1.架构复杂度(高性能、高可用、拓展、低成本安全规模)和设计原则(合适、简单、演化)






---------------------------------------------------------



10 | 架构设计流程：识别复杂度


架构设计第 1 步：识别复杂度

列出复杂度，根据业务、技术、团队进行排序。



识别复杂度实战

TPS :TransactionsPerSecond的缩写，也就是事务数/秒
QPS：Queries Per Second意思是“每秒查询率”
PV（Page View）访问量
UV（Unique Visitor）独立访客

是否需要高性能:天发1千万、读取1亿：（TPS平均115、QPS每秒1150）、（峰值平均三倍TPS是345、QPS是3450）； 
设定目标：一般是4倍，一般不超过10倍

是否需要高可用
是否需要高拓展

业务是否复杂
技术团队是否符合


分析系统的复杂性:耦合、业务逻辑复杂还是技术复杂


架构设计原则





11 | 架构设计流程：设计备选方案

第一种常见的错误：设计最优秀的方案。
第二种常见的错误：只做一个方案。
第三种常见的错误：备选方案过于详细。



备选方案的数量以 3 ~ 5 个为最佳

备选方案的差异要比较明显


备选方案的技术不要只局限于已经熟悉的技术:视野放宽


12 | 架构设计流程：评估和选择备选方案


最终的方案的挑战:

1.每个方案都是可行的
2.没有哪个方案是完美的
3.评价标准主观性比较强


几种指导思想
1.最简派： 最简单实现 不用Elasticsearch 用mysql
2.最牛派： 淘宝用的、微信开源的、Google 出品
3.最熟派：拿锤子就用锤子
4.领导派：领导说了算


 

360度环评:
列出我们需要关注的质量属性点，
然后分别从这些质量属性的维度去评估每个方案，
再综合挑选适合当时情况的最优方案。

参与者:研发、测试、运维、还有几个核心业务的主管

属性点有：性能、可用可靠性、硬件成本、项目人力投入、复杂度、安全性、可扩展性、可运维等

遵循架构设计原则 1“合适原则”和原则 2“简单原则”


1.数量对比法
2.加权法


按优先级选择:架构师综合当前的业务发展情况、团队人员规模和技能、业务发展预测等因素，将质量属性按照优先级排序，首先挑选满足第一优先级


评估和选择备选方案实战




Kafka没用过，但是上网看了相关对比，认为阿里选择自己开发RocketMQ更多是业务的驱动，当业务更多的需要以下功能的支持时，kafka不能满足或者ActiveMQ等其他消息中间件不能满足，所以选择自己开发（RocketMQ设计的真的很牛）
1、数据可靠性
kafka使用异步刷盘方式，异步Replication
RocketMQ支持异步刷盘，同步刷盘，同步Replication，异步Replication
2、严格的消息顺序
    Kafka支持消息顺序，但是一台Broker宕机后，就会产生消息乱序
    RocketMQ支持严格的消息顺序，在顺序消息场景下，一台Broker宕机后，发送消息会失败，但是不会乱序
3、消费失败重试机制
    Kafka消费失败不支持重试
    RocketMQ消费失败支持定时重试，每次重试间隔时间顺延
4、定时消息
Kafka不支持定时消息
RocketMQ支持定时消息
5、分布式事务消息
Kafka不支持分布式事务消息
阿里云ONS支持分布式定时消息，未来开源版本的RocketMQ也有计划支持分布式事务消息
6、消息查询机制
Kafka不支持消息查询
RocketMQ支持根据Message Id查询消息，也支持根据消息内容查询消息（发送消息时指定一个Message Key，任意字符串，例如指定为订单Id）
7、消息回溯
Kafka理论上可以按照Offset来回溯消息
RocketMQ支持按照时间来回溯消息，精度毫秒，例如从一天之前的某时某分某秒开始重新消费消息






13 | 架构设计流程：详细方案设计
架构设计第 4 步：详细方案设计

Elasticsearch 来做全文搜索副本数量是 2 个、3 个还是 4 个，集群节点数量是 3 个还是 6 个等。
MySQL 分库分表，那么就需要确定哪些表要分库分表
Nginx 的负载均衡策略，备选有轮询、权重分配、ip_hash、fair、url_hash 五个，具体选哪个呢



详细设计方案阶段可能遇到的一种极端情况就是在详细设计阶段发现备选方案不可行，一般情况下主要的原因是备选方案设计时遗漏了某个关键技术点或者关键的质量属性。
详细设计发现方案不可行


1.架构师不但要进行备选方案设计和选型，还需要对备选方案的关键细节有较深入的理解。

2.通过分步骤、分阶段、分系统等方式，尽量降低方案复杂度

3.如果方案本身就很复杂，采取设计团队防止只有 1~2 个架构师、思维盲点或者经验盲区







详细方案设计实战：
1. 细化设计点 1：数据库表如何设计？
2. 细化设计点 2：数据如何复制？
3. 细化设计点 3：主备服务器如何倒换？
4. 细化设计点 4：业务服务器如何写入消息？
5. 细化设计点 5：业务服务器如何读取消息？
6. 细化设计点 6：业务服务器和消息队列服务器之间的通信协议如何设计？




1、发送端和消费端如何寻址
2、发送端消息重试
3、消息消费采用pull还是push？
4、消息重复问题
5、顺序消息
6、定时消息
7、事务消息



架构设计流程




14 | 高性能数据库集群：读写分离



读写分离的基本原理是：将数据库读写操作分散到不同的节点上


读写分离的基本实现
1.服务器搭建主从集群，一主多从。
2.主机负责读写操作，从机只负责读操作。
3.主机通过复制将数据同步到从机。
4.写操作发给主机，将读操作发给从机。


问题1：主从复制延迟
1. 写操作后的读操作指定发给数据库主服务器
2. 读从机失败后再读一次主机
3. 关键业务读写操作全部指向主机，非关键业务采用读写分离

问题2：分配机制
程序代码封装和中间件封装。

1. 程序代码封装 Hibernate、淘宝的 TDDL

2. 中间件封装MySQL Proxy、MySQL Router、360 Atlas






15 | 高性能数据库集群：分库分表


单个数据库服务器问题:
1.数据量太大，读写的性能会下降，即使有索引，索引也会变得很大，性能同样会下降。
2.数据库备份和恢复需要耗费很长时间
3.数据文件越大，极端情况下丢失数据的风险越高 (机房火灾导致)


I.业务分库:按照业务模块将数据分散到不同的数据库服务器
用户、商品、订单三个业务模块
问题:
1:join 操作问题

2. 事务问题 MySQL 的 XA 性能实在太低

3. 成本问题 多台机器



I.分表
1.垂直分表 nickname描述等独立
缺点需要查询多表

2.水平分表
单表行数超过千万就有隐患

路由 ：
范围路由：分布不均匀 （建议分段大小在 100 万至 2000 万之间）；扩充简单
Hash 路由：分布比较均匀；缺点是扩充新的表很麻烦
配置路由：配置路由就是路由表，用一张独立的表来记录路由信息

join 操作
count() 操作
记录数表：记录数表
order by 操作


实现方法
1.程序代码封装
2.中间件封装






16 | 高性能NoSQL

关系数据库缺点:
1.关系数据库存储的是行记录，无法存储数据结构
2.关系数据库的 schema 扩展很不方便
3.关系数据库在大数据场景下 I/O 较高
4.关系数据库的全文搜索功能比较弱

因此我们不能盲目地迷信 NoSQL 是银弹，而应该将 NoSQL 作为 SQL 的一个有力补充

NoSQL方案分为4类:
K-V存储:无法存储数据结构Redis
文档数据库：解决schema约束MongoDB:1.新增字段简单 2.历史数据不会出错 3.可以很容易存储复杂数据
列式数据库：大数据场景下的I/O问题HBase
全文搜索引擎：全文搜索性能问题Elasticsearch

1.全文搜索基本原理

正排索引示例：文章标题查询：内容
倒排索引示例:架构 文档id:1、2、3

2.全文搜索的使用方式
索引对象是单词和文档







17 | 高性能缓存架构

1.需要经过复杂运算后得出的数据，存储系统无能为力:“count(*)

2.读多写少的数据，存储系统有心无力

重复数据放内存


设计要点。

缓存穿透:缓存没有发挥作用，业务系统虽然去缓存查询数据
1.存储数据不存在

2.缓存数据生成耗费大量时间或者资源(订单分页缓存)
分页缓存的有效期设置为 1 天 
爬虫问题


缓存雪崩:缓存失效（过期）后引起系统性能急剧下降的情况

解决方法
1. 更新锁(分布式锁)ZooKeeper每次只能一个更新


2. 后台更新:前端永久






缓存热点:
缓存热点的解决方案就是复制多份缓存副本，将请求分散到多个缓存服务器上，减轻缓存热点导致的单台缓存服务器压力
明星每条微博：生成 100 份缓存

热点复制多份缓存副本









18 | 单服务器高性能模式：PPC(建进程)与TPC(建线程程)Reactor(IO多路复用)Proactor(异步网络模型)



PPC 是 Process Per Connection 的缩写
每次有新的连接就新建一个进程去专门处理这个连接的请求


TPC 是 Thread Per Connection
每次有新的连接就新建一个线程去专门处理这个连接的请求




**18 | 单服务器高性能模式：PPC与TPC**  都无法支持高并发场景，几百个连接

磁盘、操作系统、CPU、内存、缓存、网络、编程语言、架构等，每个都有可能影响系统达到高性能，
一行不恰当的 debug 日志，就可能将服务器的性能从 TPS 30000 降低到 8000；
一个 tcp_nodelay 参数，就可能将响应时间从 2 毫秒延长到 40 毫秒。


关键之一就是服务器采取的并发模型

**并发模型**

服务器如何管理连接。服务器如何处理请求

I/O 模型：阻塞、非阻塞、同步、异步。

进程模型：单进程、多进程、多线程。

《UNIX 网络编程》

单服务器高性能模式：PPC 与 TPC


**PPC 模型是 Process Per Connection**  进程每连接

每个连接一个进程，多进程.


主进程，accept后， fork进程进行读写业务处理等。



连接数没那么多的情况，例如数据库服务器、CERN httpd

弊端:
fork代价高
父子进程通信复杂
并发数量有限

prefork 提前创建进程（pre-fork）:稍微快一些，缺点还是一样的。



**TPC模型 是 Thread Per Connection**  线程每连接
TPC和PPC本质一样，PPC没有死锁，多进程互相不影响，稳定性更高。

是指每次有新的连接就新建一个线程去专门处理这个连接的请求。

线程更轻量:与进程相比，线程更轻量级，创建线程的消耗比进程要少得多；
线程共享进程内存空间(通信更容易) :同时多线程是共享进程内存空间的，线程通信相比进程通信更简单。


弊端:

高并发性能问题:
创建线程虽然比创建进程代价低，但并不是没有代价，高并发时（例如每秒上万连接）还是有性能问题。


数据共享互斥死锁问题：
无须进程间通信，但是线程间的互斥和共享又引入了复杂度，可能一不小心就导致了死锁问题。

多线程互相影响，某线程异常导致进程退出:
多线程会出现互相影响的情况，某个线程出现异常时，可能导致整个进程退出（例如内存越界）。

CPU线程调度和切换



prethread(提前创建线程)


**19 | 单服务器高性能模式：Reactor与Proactor**

PPC、TPC连接结束进程、线程销毁，资源浪费。

资源复用

阻塞方式read，无法复用，read改成非阻塞。

IO多路复用+线程池  解决PPC、TPC性能不高问题。

select、epoll、kqueue
阻塞在select函数  wait函数等



**Reactor模型** IO多路复用+事件分配
Reactor，中文是“反应堆”。
事件反应:来了一个事件我就有相应的反应

Reactor 模式也叫 Dispatcher 模式 (调度员)


I/O 多路复用统一监听事件，收到事件后分配（Dispatch）给某个进程。


**Reactor 模式有这三种典型的实现方案：**
单 Reactor 单进程 / 线程。
单 Reactor 多线程。
多 Reactor 多进程 / 线程。

例如，
Java 语言一般使用线程（例如，Netty）。
C 语言使用进程和线程都可以。

java虚拟机是一个进程，虚拟机很多线程。
C语言一般是Reactor单进程，没有必要在进程里多创建线程。


例如
Nginx 使用进程，
Memcache 使用线程。



1. 单 Reactor 单进程 / 线程  :应用场景不多: redis

Reactor单线程:
java虚拟机是一个进程，虚拟机很多线程。
C语言一般是Reactor单进程，没有必要在进程里多创建线程。


缺点:
只有一个进程，无法发挥多核 CPU 的性能；
只能采取部署多个系统来利用多核 CPU，
但这样会带来运维复杂度，本来只要维护一个系统，
用这种方式需要在一台机器上维护多套系统。

Handler 在处理某个连接上的业务时，
整个进程无法处理其他连接的事件，
很容易导致性能瓶颈。


只适用于业务处理非常快速的场景。
目前比较著名的开源软件中使用单 Reactor 单进程的是 Redis。



2. 单 Reactor 多线程

为了克服Reactor单进程 线程方案的缺点，引入多进程  多线程显而易见，产生:单Reactor多线程


介绍下这个方案:

主线程中，Reactor对象通过select监控连接事件，收到事件后通过dispatch进程分发。

如果是连接建立的事件，则由acceptor处理，acceptor通过accept接受连接，并创建一个Handler来处理连接后续的各种事件。

如果不是连接建立事件，则Reactor会调用连接对应的handler来进行响应。

Handler只负责响应事件，不进行业务处理；Handler通过read读取到数据后，会发给Processor进行业务处理。

Processor会在独立的子线程中完成真正的业务处理，然后将响应结果发给主进程的
handler处理；handler收到响应后通过send将响应结果返回给client



单Reator多线程方案能够充分利用多核多CPU处理能力，但同时也存在下面的问题:

多线程数据共享和访问比较复杂。例如，子线程完成业务处理后，要把结果传递给主线程的
Reactor进行发送，这里涉及共享数据的互斥和保护机制。  以java的NIO为例，
Selector是线程安全的，但是通过Selector.selectKeys()返回的键的集合是非线程安全的，
对selected keys处理必须但线程处理或采取同步措施进行保护。

Reactor承担所有事件的监听和响应，只在主线程中运行，瞬间并发时会成为性能瓶颈。

你可能会发现，我只列出了 单Reactor多线程方案，没有列出 单Reactor多进程方案，
这是什么原因呢？ 主要原因在于如果采用多进程，子进程完成业务处理后，将结果返回给
父进程，并通知父进程发送给哪个client，这是很麻烦的事情。因为父进程只是通过
Reactor监听各个连接上的事件然后进程分配，子进程与父进程通信时并不是一个连接。
如果要将父进程和子进程之间的通信模拟为一个连接，并加入Reacot进行监听，则是比较复杂的。
而采用多线程时，因为多线程时共享数据的，因此线程间通信时非常方便的。虽然要额外
考虑线程间共享数据时的同步问题，但这个复杂的比进程间通信的复杂的要低很多。



3. 多 Reactor 多进程 / 线程 : Nginx、memcahce和netty


为了解决单Reactor多线程的问题，最直观的方法就是将单个Reactor该位多Reactor，这就
产生了第三个方案:多Reactor多进程/线程。


方案详细说明如下:
父进程中mainReactor 对象通过 select 监控连接建立事件，收到事件后通过Acceptor 接收， 将新的连接分配给某个子进程。

子进程的subReactor将 mainReactor分配的连接加入连接队列进行监听，并创建一个handler用于处理 连接的各种事件。

当有新的事件发生时，subReactor会调用连接对应的Handler来进程响应 handler  完成 read> 业务处理 > send的完整业务流程。


多Reactor多线程、多进程的方案看起来 比单reactor多线程要复杂，单实际实现时反而更加简单，主要原因。

父进程和子进程的职责非常明确，父进程只负责接收新连接，子进程负责完成行后续业务处理。

父子进程交互很加单，父进程只需要把新链接传给子进程，子进程无需返回数据。

子进程之间是互相独立的，无须同步共享之类的处理。

Nginx多Reactor多进程 Memcache和netty


Nginx采用的是多Reactor多进程的模式，但方案与标准的多Reactor多进程有差异。
具体差异表现为主进程中仅仅创建了监听端口，并没有创建mainReactor来accept连接，




**Proactor 前摄式**主动  异步IO
Proactor 中文翻译为“前摄器”比较难理解，与其类似的单词是 proactive，含义为“主动的”。

Reactor 是非阻塞同步网络模型，因为真正的read和send操作都需要用户进程同步操作。

这里的"同步"是指用户进程执行read和sent这类IO操作(内核态>用户态),如果把
IO操作改为异步就能进一步提升性能，这就是异步网络模型 Proactor

Reactor来事件我通知你，你处理。
Proactor来事件我处理，处理完通知你。

"我" 是内核
"事件" 就是有新链接、有数据可读、有数据可写 这些事件。
"你" 我们的程序代码


Proactor方案

Proactor Initiator 负责创建Proactor和handler 并将 Proactor 和handler都通过
Asynchronous Operation Processor 注册到内核。

Asynchronous Operation Processor 负责处理注册请求，并完成IO操作。

Asynchronous Operation Processor 完成IO操作后通知 Proactor

Proactor 根据不同的事件类型回调不同的Handler进行业务处理。

Handler 完成业务处理，Handler也可以注册新的Handler到内核进程。


理论上Proactor 比 Reactor 效率要高一些，异步IO能够充分利用DMA 特性，
让IO操作与计算重叠，但要实现真正的异步IO，操作系统需要做大量的工作。
目前Windows下通过IOCP实现了真正的异步IO，而在Linux系统下的AIO





20 | 高性能负载均衡：分类及架构和算法



任务分配器负载均衡

负载均衡分类：
DNS负载均衡
硬件负载均衡
软件负载均衡

DNS 负载均衡
1.简单、成本低 2.就近访问，提升访问速度

缺点：更新不及时、扩展性差根据业务拓展、策略太简单
更新不及时:
扩展性差:DNS 负载均衡的控制权在域名商那里，无法根据业务特点针对其做更多的定制化功能和扩展特性。
分配策略比较简单:DNS 负载均衡支持的算法少；不能区分服务器的差异（不能根据系统与服务的状态来判断负载）；也无法感知后端服务器的状态。


硬件负载均衡:
1.功能强大：全面支持各层级的负载均衡，支持全面的负载均衡算法，支持全局负载均衡
2.性能强大：对比一下，软件负载均衡支持到 10 万级并发已经很厉害了，硬件负载均衡可以支持 100 万以上的并发。
3.稳定性高：商用硬件负载均衡，经过了良好的严格测试，经过大规模使用，稳定性高。
6.支持安全防护：硬件均衡设备除具备负载均衡功能外，还具备防火墙、防 DDoS 攻击等安全功能。

优点:功能强(各层个算法)、性能强100W+、稳定(生产)、安全防护(防火、DDOS)


缺点:贵、扩展差(可配置、无法定制)
价格昂贵
扩展能力差：硬件设备，可以根据业务进行配置，但无法进行扩展和定制。


软件负载均衡:

Nginx 和 LVS
Nginx 大概能到 5 万 / 秒
LVS 的性能是十万级，据说可达到 80 万 / 秒
F5 性能是百万级，从 200 万 / 秒到 800 万 / 秒都有


软件负载均衡的优点：

简单、便宜、灵活

缺点:
性能一般：一个 Nginx 大约能支撑 5 万并发。
功能没有硬件负载均衡那么强大。
一般不具备防火墙和防 DDoS 攻击等安全功能。


组合的基本原则为：DNS负载均衡用于实现地理级别的负载均衡；硬件负载均衡用于实现集群级别的负载均衡；软件负载均衡用于实现机器级别的负载均衡。


DNS负载均衡地理级别>硬件负载集群级别>软件负载机器级别
机器级别





21 | 高性能负载均衡：算法

任务平分类：负载均衡系统将收到的任务平均分配给服务器进行处理，这里的“平均”可以是绝对数量的平均，也可以是比例或者权重上的平均。

负载均衡类：负载均衡系统根据服务器的负载来进行分配，这里的负载并不一定是通常意义上我们说的“CPU 负载”，而是系统当前的压力，可以用 CPU 负载来衡量，也可以用连接数、I/O 使用率、网卡吞吐量等来衡量系统的压力。

性能最优类：负载均衡系统根据服务器的响应时间来进行任务分配，优先将新任务分配给响应最快的服务器。

Hash类：负载均衡系统根据任务中的某些关键信息进行 Hash 运算，将相同 Hash 值的请求分配到同一台服务器上。常见的有源地址Hash、目标地址 Hash、session id hash、用户 ID Hash 等。



轮询:按照顺序轮流分配到服务器(新老机器、是否100不关心)

加权轮询:解决服务器处理能力不一致


负载最低优先:
LVS 这种 4 层连接数
Nginx 这种 7 层HTTP 请求数
CPU 负载I/O 负载




性能最优类(负载最低优先类算法类似)采样率、周期10秒内性能最优



Hash 类 源地址 Hash ID Hash





----------------------------------------------------------------------------------------------


22 | 想成为架构师，你必须知道CAP理论

# I.CAP 定理（CAP theorem）又被称作布鲁尔定理（Brewer's theorem）

CAP 关注的粒度是数据，而不是整个系统。
CAP前提是系统发生了“分区”现象。如果系统没有发生分区现象，也就是说 P 不存在的时候（节点间的网络连接一切正常），
我们没有必要放弃 C 或者 A，应该 C 和 A 都可以保证，这就要求架构设计的时候既要考虑分区发生时选择 CP 还是 AP，
也要考虑分区没有发生时如何保证 CA。

-----------------------------------过时第一版CAP开始--------------------------------------
# II.第一版(过时，不准确)   outdated
 III.cap定理:(过时，不准确)   outdated
对于一个分布式计算系统，不可能同时满足一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）三个设计约束。

一致性（Consistency）过时:所有节点在同一时刻都能看到相同的数据

可用性（Availability）过时：每个请求都能得到成功或者失败的响应。

一致性（Consistency）过时:所有节点在同一时刻都能看到相同的数据

------------------------------------过时第一版CAP结束---------------------------------------

# II.第二版
# I.cap定理:
在一个分布式系统（指互相连接并共享数据的节点的集合）中，当涉及读写操作时，只能保证一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）
三者中的两个，另外一个必须被牺牲。


区别:
1.分布式系统并不一定会互联和共享数据   : (强调了两点：interconnected互联 和 share data共享数据)

例:memcache集群，没有连接和共享数据，不符合CAP理论


2. CAP关注的是对数据的读写操作，而不是所有功能

例: ZooKeeper选举机制不在探讨范围


##1 一致性（Consistency）:  
第一版 过时：所有节点在同一时刻都能看到相同的数据
第二版    ： **对某个指定的客户端来说，读操作保证能够返回最新的写操作结果**

区别:
第一版从节点 node 的角度描述，第二版从客户端 client 的角度描述。
第一版的关键词是 see，第二版的关键词是 read。
 

##2 可用性（Availability）：
第一版  过时:一致性（Consistency）过时:所有节点在同一时刻都能看到相同的数据
第二版     : **非故障的节点在合理的时间内返回合理的响应（不是错误和超时的响应）**


区别:
第一版是 every request每个请求，第二版强调了 A non-failing node非故障节点。 
every request 是不严谨的，因为只有非故障节点才能满足可用性要求，如果节点本身就故障了，发给节点的请求不一定能得到一个响应。


第一版的 success/failure 的定义太泛了，超时也算失败、错误也算失败、异常也算失败、结果不正确也算失败。
第二版的解释明确了不能超时、不能出错，结果是合理的，注意没有说“正确”的结果。

例如(合理的数据)，应该返回 100 但实际上返回了 90，肯定是不正确的结果，但可以是一个合理的结果。



##3 分区容忍性（Partition Tolerance）： 
第一版  过时：出现消息丢失或者分区错误时系统能够继续运行。
第二版     : **当出现网络分区后，系统能够继续“履行职责”**

区别:
第一版用的是 work，第二版用的是 function。work 强调“运行”，只要系统不宕机，我们都可以说系统在 work，返回错误也是 work，拒绝服务也是 work；
而 function 强调“发挥作用”“履行职责”，这点和可用性是一脉相承的。也就是说，只有返回 reasonable response合理相应 才是 function。相比之下，第二版解释更加明确。

第一版描述分区用的是 message loss or partial failure，第二版直接用 network partitions。
对比两版解释，第一版是直接说原因，即 message loss 造成了分区，但 message loss 的定义有点狭隘，因为通常我们说的 message loss（丢包），只是网络故障中的一种；
第二版直接说现象，即发生了分区现象，不管是什么原因，可能是丢包，也可能是连接中断，还可能是拥塞，只要导致了网络分区，就通通算在里面。


I.CAP 应用
网络必然会出现问题(P分区容错性：出现分区系统能继续履行职责)必然会存在，所以不存在CA。( C一致性，写入时，由于出现分区，必然会失败，所以又和A可用性冲突了)


II.CP  一致性+分区容错性

II.AP 可用性+分区容错性

例:

N1(data=X)节点 和 N2(data=X)节点 :数据都是X
N1(data=Y)节点 和 N2(data=X)节点 :N1节点数据并未同步到N2节点

CP：  客户端访问 N2节点， 返回失败  没有A可用性。  (不返回正确数据) 不是最新的数据不返回正常。
AP：  客户端访问 N2节点， 返回X    没有C一致性。    (虽然不是正确数据，但是返回合理数据) 返回不是最新的数据



I.zookeeper CP和 eureka AP
II.zookeeper :CP
优点:数据最终一致
缺点:选举时不可用
适用场景:数据一致性要求比较高

II.eureka :AP
优点:高可用
缺点:服务节点数据可能不一致
适用场景:对注册中心服务可用性要求较高

-------------------------------Base理论开始-----------------------------------------
BASE 是指基本可用（Basically Available）、
软状态（ Soft State）、
最终一致性（ Eventual Consistency），核心思想是即使无法做到强一致性（CAP 的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性。

1. 基本可用（Basically Available）
   分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。
   
账户系统中:登录是核心、注册相对不核心。可以让登录可用，注册不可用。

2. 软状态（Soft State）
   允许系统存在中间状态，而该中间状态不会影响系统整体可用性。这里的中间状态就是 CAP 理论中的数据不一致。

3. 最终一致性（Eventual Consistency）
   系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。

一定时间和最终: 微博部分用户延迟看到明星微博

是对 CAP 中 AP 方案的一个补充

CAP 理论是忽略延时的，而实际应用中延时是无法避免的。

BASE 是 CAP 理论中 AP 方案的延伸。








-------------------------------Base理论结束-----------------------------------------




分布式环境:CAP理论：只能保证三者中的两个


一致性（Consistence）:所有节点同一时刻都能看到相同的数据
可用性（Availability）:非故障的节点在合理的时间内返回合理的响应（不是错误和超时的响应）
分区容错性（Partition Tolerance）:出现网络分区(脑裂)，系统能够继续“履行职责”|消息丢失或分区错误系统能够继续运行

数据相同又可用，不可能出现脑裂，无P
数据相同，一份无副本。故障时，不可用 无A
多节点可用、丢失数据继续运行，无C

CP:返回错误error

AP:返回旧数据X





分布式：
高可用
CAP理论(一致性、可用性、分区容错)
三选二

FMEA排除可用隐患


可拓展

可拓展基本思想
面向流程拆分 面向服务拆分 面向功能拆分


流程拆分（分层架构）：MVC（展示层 控制 数据）


服务拆分（SOA、微服务）： 注册服务

功能拆分（微内核架构）：
注册服务：提供多种方式进行注册，包括手机号注册、身份证注册、学生邮箱注册三个功能。
登录服务：包括手机号登录、身份证登录、邮箱登录三个功能。
安全设置服务：包括修改密码、安全手机、找回密码等功能。


业务逻辑、可扩展、可靠性、性能拆分


服务发现、路由、容错、接口框架、API 网关




稳定服务，迭代变动服务






