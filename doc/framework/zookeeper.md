

**I.简述CAP理论**


**集群：**
不同服务器部署同一套应用服务对外提供访问，实现服务的负载均衡或者互备(热备，主从等)，
指同一种组件的多个实例，形成的逻辑上的整体。单个节点可以提供完整服务。集群是物理形态



**分布式：**
服务的不同模块部署在不同的服务器上，单个节点不能提供完整服务，
需要多节点协调提供服务(也可以是相同组件部署在不同节点、但节点间通过交换信息协作提供服务)，分布式强调的是工作方式


**SOA：**
面向服务的架构，一种设计方法，其中包含多个服务， 服务之间通过相互依赖最终提供一系列的功能。
一个服务通常以独立的形式存在于操作系统进程中。各个服务之间通过网络调用。

中心化实现：ESB(企业服务总线)，各服务通过ESB进行交互，解决异构系统之间的连通性，
通过协议转换、消息解析、消息路由把服务提供者的数据传送到服务消费者。很重，有一定的逻辑，可 以解决一些公用逻辑的问题。

去中心化实现：微服务


**微服务：**
在 SOA 上做的升华，微服务架构强调的一个重点是业务需要彻底的组件化和服务化，原有的单个业务系统会拆分为多个可以独立开发、设计、运行的小应用。这些小应用之间通过服务完成交互和 集成

服务单一职责
轻量级通信：去掉ESB总线，采用restapi通信



**I.简述CAP理论**

数据一致性(consistency)：如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据，对调用者而言数据具有强一致性(strong consistency)
服务可用性(availability)：所有读写请求在一定时间内得到响应，可终止、不会一直等待
分区容错性(partition-tolerance)：在网络分区的情况下，被分隔的节点仍能正常对外服务


**对某个指定的客户端来说，读操作保证能够返回最新的写操作结果**
**非故障的节点在合理的时间内返回合理的响应（不是错误和超时的响应）**
**当出现网络分区后，系统能够继续“履行职责”**

# II.不存在CA模式

如果选择了 CA 而放弃了 P，那么当发生分区现象时，为了保证 C，系统需要禁止写入，当有写入请求时，系统返回 error（例如，当前系统不允许写入），
这又和 A 冲突了，因为 A 要求返回 no error 和no timeout。
因此，分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。

反证：
如果 CAP 三者可同时满足，由于允许 P 的存在，则一定存在节点之间的丢包，如此则不能保证 C
因为允许分区容错，写操作可能在节点 1 上成功，在节点 2 上失败，这时候对于 Client 1 (读取节点1)
和 Client 2(读取节点2)，就会读取到不一致的值，出现不一致的情况。
如果要保持一致性，写操作必须同时失败， 也就是降低系统的可用性。

# II.base理论补充 CAP

cap理论的一种妥协，由于cap只能二取其一，base理论降低了发生分区容错时对可用性和一致性的要求
1、基本可用：允许可用性降低（可能响应延长、可能服务降级），
2、软状态：指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性。
2、最终一致性：节点数据同步可以存在时延），但在一定的期限后必须达成数据的一致，状态变为最终状态




# II.数据一致性模型有哪些
III.强一致性：
当更新操作完成之后，任何多个后续进程的访问都会返回最新的更新过的值，这种是对用户最友好的，就是用户上一次写什么，下一次就保证能读到什么。
根据 CAP 理论，这种实现需要牺牲可用性。

III.弱一致性：
系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。
用户读到某一操作对系统数据的更新需要一段时间，我们称这段时间为“不一致性窗口”。


III.最终一致性：
最终一致性是弱一致性的特例，强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。
因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。
到达最终一致性的时间 ，就是不一致窗口时间，在没有故障发生的前提下，不一致窗口的时间主要受通信延迟，系统负载和复制副本的个数影响。


最终一致性模型根据其提供的不同保证可以划分为更多的模型，包括因果一致性和会话一致性等。

III.因果一致性：
要求有因果关系的操作顺序得到保证，非因果关系的操作顺序则无所谓。


进程 A 在更新完某个数据项后通知了进程 B，
那么进程 B 之后对该数据项的访问都应该能够获取到进程A 更新后的最新值，
并且如果进程 B 要对该数据项进行更新操作的话，务必基于进程 A 更新后的最新值。


III.会话一致性：
将对系统数据的访问过程框定在了一个会话当中，约定了系统能保证在同一个有效的会话
中实现“读己之所写”的一致性，就是在你的一次访问中，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。
实际开发中有分布式的 Session 一致性问题，可以认为是会话一致性的一个应用。



# 选举算法  Quorum(法定人数)机制、WARO协议（Write All Read one ）

II.WARO(Write All Read one副本控制协议)：都写成功才算成功
写操作时、只有当所有的副本都更新成功之后，这次写操作才算成功，否则视为失败。
优先保证读、任何节点读到的数据都是最新数据，牺牲了更新服务的可用性、只要有一个副本宕机了，写服务就不会成功。
但只要有一个节点存活、仍能提供读服务。


II.Quorum机制(法定人数)：共10个，写3个成功，读8个。
10个副本，一次成功更新了三个，那么至少需要读取八个副本的数据，可以保证读到了最新的数据。

无法保证强一致性，也就是无法实现任何时刻任何用户或节点都可以读到最近一次成功提交的副本数据。
需要配合一个获取最新成功提交的版本号的 metadata 服务，这样可以确定最新已经提交的版本号，然后从已经读到的数据中就可以确认最新写入的数据

Quorum 的定义如下：假设有 N 个副本，更新操作 wr 在 W 个副本中更新成功之后，则认为此次更新操作 wr 成功，把这次成功提交的更新操作对应的数据叫做：“成功提交的数据”。对于读操作而言，至少需要读R个副本，其中，W+R>N ，即 W 和 R 有重叠，一般，W+R=N+1。
N = 存储数据副本的数量
W = 更新成功所需的副本
R = 一次数据对象读取要访问的副本的数量



# paxos算法(帕克索斯):
Paxos算法解决的是一个分布式系统如何就某个值（决议）达成一致。
一个典型的场景是，在一个分布式数据库系统中，如果各个节点的初始状态一致，每个节点执行相同的操作序列，那么他们最后能够得到一个一致的状态。


为了保证每个节点执行相同的操作序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。
在Paxos算法中，有三种角色：Proposer (提议者),Acceptor（接受者），Learners（记录员）



paxos算法解决的是一个分布式系统如何就某个值达成一致。

一个典型的场景是，在一个分布式数据库系统中，如果各个节点的初始状态一致，每个节点
执行相同的操作序列，那么他们最后能够得到一个一致的状态。 为了保证每个节点的初始状态一致，
每个节点执行相同的操作序列，那么他们最后能够得到一个一致的状态。
为了保证每个节点执行相同的操作蓄力了，需要在每一条执行上执行一个一致性算法，
 以保证每个节点看到的指令一致。
在paxos算法中， 有三种角色:
Proposer 提议者，acceptor 接受者、learners 记录员

**proposer提议者：** 只要proposer发的天 propose被半数以上的acceptor接受，proposer就认为
提案的value 被选定了。

**acceptor接受者:** 只要acceptor 接受了某提案，acceptor就认为该天的value被选定了

**learner记录者:** acceptor告诉learner那个value被选定，learner就认为那个value被选定。


paxos算法分为两个阶段，具体如下:

阶段1 prepare
1.proposer 收到client 请求或者发现本地有未提交的值，选择一个提案编号N，然后向半数以上的
acceptor发送编号为N的prepare请求。

2.acceptor收到一个编号为N的 prepare请求， 如果该轮paxos
2.1 本节点已经有已提交的value记录，对比记录的编号和N ，大于N则拒绝回应，否则返回该记录value和编号
2.2  没有已提交记录，判断本地是否有编号N1，  N1>N,拒接响应， 否则将N1 改为N （没有N1 记录N），并响应。


阶段2 accept
如果proposer 收到半数以上acceptro对其发出编号为N的 prepare请求和响应，那么它就会发送一个
针对NV 天的 accept请求给半数以上的acceptor.V 就是收到响应中编号最大的value,
如果响应中不包含任何value，那么V就由proposer自己决定。


#Paxos(帕克索斯) 总结:  分布式一致性算法  理论
https://www.jianshu.com/p/a7c6a9f4226a

I.角色组成
Proposer(提议者) :  (N ,V ):N版本号、V数据
Acceptor(接受者) : (ResN ,AcceptN,AcceptV ):ResN历史版本号，AcceptN历史版本号，AcceptV历史数据
Learner(记录者)  :

I.阶段和请求名称
第一阶段:Prepare准备 请求
第二阶段:Accept同意 请求


I.流程
第一阶段:Prepare准备请求

1.提议者(Proposer)发送自增的**提案编号N**的Prepare准备请求 到**半数以上**的接受者(Acceptor).

2.接受者(Acceptor)收到请求后：
2.1.接受者: 本地编号  <  N  : 本地编号变成N，返回历史数据(Pok ,AcceptN,AcceptV )
2.2.接受者: 本地编号  >= N  :不响应或响应error



第二阶段:Accept同意请求

1.提议者(Proposer) ：半数以上的返回，发送 [N,V]提案的Accept同意请求给半数以上 接受者(Acceptor)
注意：V是收到的响应中编号最大的提案的value，如果响应中不包含任何提案，那么V由提议者(Proposer)决定。

2.接受者(Acceptor)收到请求： 
2.1.接受者: 没有接收过大于N的Prepare请求，就接受这个提案。
2.1.接受者: 接收过更大的Prepare请求，不响应或响应error。



如果响应过半，确定V被选定。
如果不过半， 重新发起Prepare请求

I.Learner(记录者) 学习的三种方案

II.Acceptor(接受者)接受提案发送给Learner(记录者)方式:
1.将该提案发送给 **所有Learner(记录者**) 。 
优点:快速获取被选定的value
缺点:通信次数为M*N

2.将该提案发送给 **主Learner(记录者)** ，主记录者通知其他Learner(记录者)。
优点:通信次数减少(M+N-1)
缺点:单点问题,主故障

3.将该提案发送给 **Learner(记录者)集合**，主记录者通知其他Learner(记录者)。
优点:集合节点越多，可靠性越好
缺点:网络通信复杂度越高。


I.Paxos算法的活性 (活锁问题)
多个Proposer(提议者) ,同时提出递增提案，陷入活锁死循环。

选取主Proposer(提议者):既能保证安全性，又能保证活性的分布式一致性算法。




raft算法
4：Raft
5：ZAB

https://www.cnblogs.com/yxy-ngu/p/12587341.html#_label1_2








# I. 一致性Hash算法原理

一致性hash(增加删除节点不会大面积影响)
https://www.jianshu.com/p/528ce5cd7e8f


II.定义等
目的是解决分布式缓存的问题
在移除或者添加一个服务器时，能够尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系。
一致性哈希解决了简单哈希算法在分布式哈希表( Distributed Hash Table，DHT) 中存在的动态伸缩等问题

II.工作原理
构造环
对存储节点的哈希值进行计算，其将存储空间抽象为一个环，将存储节点配置到环上。
环上所有的节点都有一个值

对数据进行哈希计算，按顺时针方向将其映射到离其最近的节点上去

当有节点出现故障离线时，按照算法的映射方法，受影响的仅仅为环上故障节点开始逆时针方向至下一个节点之间区间的数据对象，而这些对象本身就是映射到故障节点之上的。


当有节点增加时，比如，在节点A和B之间重新添加一个节点H，受影响的也仅仅是节点H逆时针遍历直到B之间的数据对象，将这些重新映射到H上即可


**II.讲解：构造环，上只有ABC三个节点**

**请求1来时：**
命中到环的A-B区间，就请求到B。
B-C区间，请求到B 。 
C-A区间，请求A节点。


**节点B挂了，AC正常**
命中到环的A-B区间，就请求到C。 只有C节点会受影响。

**新增H节点，ABC正常**
C-H区间，数据请求到H。 只有C节点会受影响






# I.简述 raft 算法  （相当于paxos一种实现）       分布式一致性算法

paxos没有leader
raft先选一个leader

官网
https://raft.github.io/
http://blog.itpub.net/31556438/viewspace-2637112/
https://www.jianshu.com/p/8e4bbe7e276c

II.角色
在任何给定时间，每个服务器都处于以下三种状态之一，领导者（Leader），追随者（Follower）或候选人（Candidate）。 这几个状态见可以相互转换。


**Leader：**
处理所有客户端交互，日志复制等，一般一次只有一个Leader

**Follower：**
类似选民，完全被动

**Candidate：候选人**
类似Proposer律师，可以被选为一个新的领导人 


II.选举Leader
每次选举Term 都自增

III.初始化时
1.当服务器启动时，它们以Follower的身份开始， 随机超时，变成candidate，发起选举。
2.服务器从Leader或Candidate接收到有效的RPC请求，服务器就会保持Follower状态。
3.Leader向所有Follower发送定期心跳（不带日志条目的AppendEntries RPC）以保持其权限。
4.如果一个Follower在称为选举超时的一段时间内没有接到任何通信，该Follower认为没有可行的领导者并开始选举新的Leader。



III.Leader异常时选主：(流程和初始化一样)
老leader复活时，自动降级为follower

第几轮选举是有记录的，重新加入的 Leader 是第一轮选举 (Term 1) 选出来的，
而现在的 Leader 则是 Term 2，所有原来的 Leader 会自觉降级为 Follower



**II.日志复制（Log Replication）**

Leader 接收来自客户端的请求并将其以日志条目的形式复制到集群中的其它节点，并且强制要求其它节点的日志和自己保持一致；

数据状态:
Uncommitted 未提交 未确认
Committed    提交 确认

**III.数据复制流程**
Leader 1个
Follower 3个

正常数据复制流程：
1. L(Leader)收到客户端请求 data(sally) , L写入本地日志，数据状态是Uncommitted。
2.L给三个F(Follower)发送 AppendEntries请求, 三个F记录到本地日志并返回OK，数据状态是Uncommitted。
3.L收到过半的OK(包含L自己和F)，L数据状态改为Committed状态，并返回客户端OK。
4.当L再次给F发动AppendEntries请求， F数据状态修改成Committed。
   

数据分区时：
以最终Leader数据为准，异常L保存的数据会删除。






II.安全性（Safety） ：如果有任何的服务器节点已经应用了一个确定的日志条目到它的状态机中，那么其它服务器节点不能在同一个日志索引位置应用一个不同的指令。





# I. ZAB:(  Zookeeper Atomic Broadcast :Zookeeper 原子广播)
  
ZAB 协议全称：Zookeeper Atomic Broadcast（Zookeeper 原子广播协议）。

Zookeeper 是一个为分布式应用提供高效且可靠的分布式协调服务。在解决分布式一致性方面，Zookeeper 并没有使用 Paxos ，而是采用了 ZAB 协议。


ZAB 协议定义：ZAB 协议是为分布式协调服务 Zookeeper 专门设计的一种支持 崩溃恢复 和 原子广播 协议。


https://www.cnblogs.com/shuaiandjun/p/9383655.html
https://zookeeper.apache.org/doc/r3.7.0/zookeeperAuditLogs.html
https://dl.acm.org/doi/10.1145/1529974.1529978
官网译文
http://www.javashuo.com/article/p-ebqcodox-he.html
https://www.cnblogs.com/j-well/p/7061091.html

II.zab对比 raft请求
raft leader 读写都能处理

zab
只处理写请求
读取从节点，会先调用syn函数


# II.zab节点的三种状态：
following：服从leader的命令
leading：负责协调事务
election选举/looking：选举状态


election epoch 选举时期:


 
# **II。Zab协议包括两个模式：**
恢复（recovery） 和广播（broadcast）
启动和异常时处于：恢复模式
 

# III.广播模式：
二阶段提交（two-phase commit）
广播协议使用FIFO（TCP）通道进行所有通信
单调递增ID：zxid

流程：Leader 生成zxid通过 FIFO通道 发送给 Follower

1.leader 生成一个zxid 通过FIFO通道 发送给所有 follower
2.follower 将zxid记入磁盘，并 通过FIFO通道 反馈给 leader ack
3.Leader 收到大部分ack时 ，通过FIFO通道 发送commit指令，以后本地提交消息。
4.Follower 收到COMMIT指令后也提交消息。

# III.恢复（recovery）模式
https://baijiahao.baidu.com/s?id=1666465070459184658&wfr=spider&for=pc

**1.leader选取**

ABC三个节点选举：

A申请成为Leader:
第一步：成员A告诉BC说我要成为老大，BC记录下来。（A成员广播）
第二步：B回复可以，C回复不可以。（B成员广播）
第三步：A和C收到B的消息，更新自己的记录表。
此时A：2票，B：0票，C：0票。

C申请成为Leader:
第四步：C这时候不满意了，也要选举成为老大。而且还给自己投了一票。
第五步：A回复可以，B回复可以。更新自己的记录表。
第六步：C收到AB的回复，更新。
此时A：0票，B：0，C：3票。于是确定C就是下一届组织老大了。

**2.崩溃恢复**
要求： 
第一： 确保已经被leader提交的proposal必须最终被所有的follower服务器提交。 
第二：确保丢弃已经被leader发出的但是没有被提交的proposal。


第一步：选取当前取出最大的ZXID，代表当前的事件是最新的。
第二步：新leader把这个事件proposal提交给其他的follower节点
第三步：follower节点会根据leader的消息进行回退或者是数据同步操作。最终目的要保证集群中所有节点的数据副本保持一致。





# I.zk选主

https://www.cnblogs.com/shuaiandjun/p/9383655.html


II.zookeeper 三种选主方式：
 LeaderElection  
 AuthFastLeaderElection
 FastLeaderElection （最新默认）


II.选举流程简述
1、2、3、4、5依次启动 ：最终3为leader。
刚启动都是Looking状态

服务器1启动，给自己投票，然后发投票信息，由于其它机器还没有启动所以它收不到反馈信息，服务器1的状态一直属于Looking(选举状态)。
服务器2启动，给自己投票，同时与之前启动的服务器1交换结果，由于服务器2的编号大所以服务器2胜出，但此时投票数没有大于半数，所以两个服务器的状态依然是LOOKING。
服务器3启动，给自己投票，同时与之前启动的服务器1,2交换信息，由于服务器3的编号最大所以服务器3胜出，此时投票数正好大于半数，所以服务器3成为领导者，服务器1,2成为小弟。
服务器4启动，给自己投票，同时与之前启动的服务器1,2,3交换信息，尽管服务器4的编号大，但之前服务器3已经胜出，所以服务器4只能成为小弟。
服务器5启动，后面的逻辑同服务器4成为小弟


II.名词


1、Serverid：服务器ID
比如有三台服务器，编号分别是1,2,3。
编号越大在选择算法中的权重越大。

2、Zxid：数据ID  64位
服务器中存放的最大数据ID

值越大说明数据越新，在选举算法中数据越新权重越大。

前32位 leader自增ID + 后32位 事务ID 自增

3、Epoch：逻辑时钟 ：投票的次数
Epoch会随着选举轮数的增加而递增


4、Server状态：选举状态

LOOKING，竞选状态。
FOLLOWING，随从状态，同步leader状态，参与投票。
OBSERVING，观察状态,同步leader状态，不参与投票。
LEADING，领导者状态。

投票信息包含 ：Serverid(服务器ID)、 Zxid(数据ID)、Epoch(逻辑时钟)、Server状态
  

八、选举流程详述

收到一个选主请求
I.判断Epoch选主次数，
II.收到的比本节点的大，
1.判断Epoch投票次数，以大的为准()

II.收到请求的节点是( LOOKING 状态)
判断流程Epoch(逻辑时钟) >Zxid(数据ID) >相等> Serverid(服务器ID)


II.收到请求的节点是( Following 、Leading状态)
判断流程Epoch(逻辑时钟) 如果相等就保持

如果不相等就把自身变为 LOOKING 状态 













