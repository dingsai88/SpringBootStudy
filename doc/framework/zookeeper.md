

**I.简述CAP理论**


**集群：**
不同服务器部署同一套应用服务对外提供访问，实现服务的负载均衡或者互备(热备，主从等)，
指同一种组件的多个实例，形成的逻辑上的整体。单个节点可以提供完整服务。集群是物理形态



**分布式：**
服务的不同模块部署在不同的服务器上，单个节点不能提供完整服务，
需要多节点协调提供服务(也可以是相同组件部署在不同节点、但节点间通过交换信息协作提供服务)，分布式强调的是工作方式


**SOA：**
面向服务的架构，一种设计方法，其中包含多个服务， 服务之间通过相互依赖最终提供一系列的功能。
一个服务通常以独立的形式存在于操作系统进程中。各个服务之间通过网络调用。

中心化实现：ESB(企业服务总线)，各服务通过ESB进行交互，解决异构系统之间的连通性，
通过协议转换、消息解析、消息路由把服务提供者的数据传送到服务消费者。很重，有一定的逻辑，可 以解决一些公用逻辑的问题。

去中心化实现：微服务


**微服务：**
在 SOA 上做的升华，微服务架构强调的一个重点是业务需要彻底的组件化和服务化，原有的单个业务系统会拆分为多个可以独立开发、设计、运行的小应用。这些小应用之间通过服务完成交互和 集成

服务单一职责
轻量级通信：去掉ESB总线，采用restapi通信



**I.简述CAP理论**

数据一致性(consistency)：如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据，对调用者而言数据具有强一致性(strong consistency)
服务可用性(availability)：所有读写请求在一定时间内得到响应，可终止、不会一直等待
分区容错性(partition-tolerance)：在网络分区的情况下，被分隔的节点仍能正常对外服务


**对某个指定的客户端来说，读操作保证能够返回最新的写操作结果**
**非故障的节点在合理的时间内返回合理的响应（不是错误和超时的响应）**
**当出现网络分区后，系统能够继续“履行职责”**

# II.不存在CA模式

如果选择了 CA 而放弃了 P，那么当发生分区现象时，为了保证 C，系统需要禁止写入，当有写入请求时，系统返回 error（例如，当前系统不允许写入），
这又和 A 冲突了，因为 A 要求返回 no error 和no timeout。
因此，分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。

反证：
如果 CAP 三者可同时满足，由于允许 P 的存在，则一定存在节点之间的丢包，如此则不能保证 C
因为允许分区容错，写操作可能在节点 1 上成功，在节点 2 上失败，这时候对于 Client 1 (读取节点1)
和 Client 2(读取节点2)，就会读取到不一致的值，出现不一致的情况。
如果要保持一致性，写操作必须同时失败， 也就是降低系统的可用性。

# II.base理论补充 CAP

cap理论的一种妥协，由于cap只能二取其一，base理论降低了发生分区容错时对可用性和一致性的要求
1、基本可用：允许可用性降低（可能响应延长、可能服务降级），
2、软状态：指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性。
2、最终一致性：节点数据同步可以存在时延），但在一定的期限后必须达成数据的一致，状态变为最终状态




# II.数据一致性模型有哪些
III.强一致性：
当更新操作完成之后，任何多个后续进程的访问都会返回最新的更新过的值，这种是对用户最友好的，就是用户上一次写什么，下一次就保证能读到什么。
根据 CAP 理论，这种实现需要牺牲可用性。

III.弱一致性：
系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。
用户读到某一操作对系统数据的更新需要一段时间，我们称这段时间为“不一致性窗口”。


III.最终一致性：
最终一致性是弱一致性的特例，强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。
因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。
到达最终一致性的时间 ，就是不一致窗口时间，在没有故障发生的前提下，不一致窗口的时间主要受通信延迟，系统负载和复制副本的个数影响。


最终一致性模型根据其提供的不同保证可以划分为更多的模型，包括因果一致性和会话一致性等。

III.因果一致性：
要求有因果关系的操作顺序得到保证，非因果关系的操作顺序则无所谓。


进程 A 在更新完某个数据项后通知了进程 B，
那么进程 B 之后对该数据项的访问都应该能够获取到进程A 更新后的最新值，
并且如果进程 B 要对该数据项进行更新操作的话，务必基于进程 A 更新后的最新值。


III.会话一致性：
将对系统数据的访问过程框定在了一个会话当中，约定了系统能保证在同一个有效的会话
中实现“读己之所写”的一致性，就是在你的一次访问中，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。
实际开发中有分布式的 Session 一致性问题，可以认为是会话一致性的一个应用。



# 选举算法  Quorum(法定人数)机制、WARO协议（Write All Read one ）

II.WARO(Write All Read one副本控制协议)：都写成功才算成功
写操作时、只有当所有的副本都更新成功之后，这次写操作才算成功，否则视为失败。
优先保证读、任何节点读到的数据都是最新数据，牺牲了更新服务的可用性、只要有一个副本宕机了，写服务就不会成功。
但只要有一个节点存活、仍能提供读服务。


II.Quorum机制(法定人数)：共10个，写3个成功，读8个。
10个副本，一次成功更新了三个，那么至少需要读取八个副本的数据，可以保证读到了最新的数据。

无法保证强一致性，也就是无法实现任何时刻任何用户或节点都可以读到最近一次成功提交的副本数据。
需要配合一个获取最新成功提交的版本号的 metadata 服务，这样可以确定最新已经提交的版本号，然后从已经读到的数据中就可以确认最新写入的数据

Quorum 的定义如下：假设有 N 个副本，更新操作 wr 在 W 个副本中更新成功之后，则认为此次更新操作 wr 成功，把这次成功提交的更新操作对应的数据叫做：“成功提交的数据”。对于读操作而言，至少需要读R个副本，其中，W+R>N ，即 W 和 R 有重叠，一般，W+R=N+1。
N = 存储数据副本的数量
W = 更新成功所需的副本
R = 一次数据对象读取要访问的副本的数量



# paxos算法(帕克索斯):
Paxos算法解决的是一个分布式系统如何就某个值（决议）达成一致。
一个典型的场景是，在一个分布式数据库系统中，如果各个节点的初始状态一致，每个节点执行相同的操作序列，那么他们最后能够得到一个一致的状态。


为了保证每个节点执行相同的操作序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。
在Paxos算法中，有三种角色：Proposer (提议者),Acceptor（接受者），Learners（记录员）



paxos算法解决的是一个分布式系统如何就某个值达成一致。

一个典型的场景是，在一个分布式数据库系统中，如果各个节点的初始状态一致，每个节点
执行相同的操作序列，那么他们最后能够得到一个一致的状态。 为了保证每个节点的初始状态一致，
每个节点执行相同的操作序列，那么他们最后能够得到一个一致的状态。
为了保证每个节点执行相同的操作蓄力了，需要在每一条执行上执行一个一致性算法，
 以保证每个节点看到的指令一致。
在paxos算法中， 有三种角色:
Proposer 提议者，acceptor 接受者、learners 记录员

**proposer提议者：** 只要proposer发的天 propose被半数以上的acceptor接受，proposer就认为
提案的value 被选定了。

**acceptor接受者:** 只要acceptor 接受了某提案，acceptor就认为该天的value被选定了

**learner记录者:** acceptor告诉learner那个value被选定，learner就认为那个value被选定。


paxos算法分为两个阶段，具体如下:

阶段1 prepare
1.proposer 收到client 请求或者发现本地有未提交的值，选择一个提案编号N，然后向半数以上的
acceptor发送编号为N的prepare请求。

2.acceptor收到一个编号为N的 prepare请求， 如果该轮paxos
2.1 本节点已经有已提交的value记录，对比记录的编号和N ，大于N则拒绝回应，否则返回该记录value和编号
2.2  没有已提交记录，判断本地是否有编号N1，  N1>N,拒接响应， 否则将N1 改为N （没有N1 记录N），并响应。


阶段2 accept
如果proposer 收到半数以上acceptro对其发出编号为N的 prepare请求和响应，那么它就会发送一个
针对NV 天的 accept请求给半数以上的acceptor.V 就是收到响应中编号最大的value,
如果响应中不包含任何value，那么V就由proposer自己决定。


#Paxos(帕克索斯) 总结:  分布式一致性算法
https://www.jianshu.com/p/a7c6a9f4226a

I.角色组成
Proposer(提议者) :  (N ,V ):N版本号、V数据
Acceptor(接受者) : (ResN ,AcceptN,AcceptV ):ResN历史版本号，AcceptN历史版本号，AcceptV历史数据
Learner(记录者)  :

I.阶段和请求名称
第一阶段:Prepare准备 请求
第二阶段:Accept同意 请求


I.流程
第一阶段:Prepare准备请求

1.提议者(Proposer)发送自增的**提案编号N**的Prepare准备请求 到**半数以上**的接受者(Acceptor).

2.接受者(Acceptor)收到请求后：
2.1.接受者: 本地编号  <  N  : 本地编号变成N，返回历史数据(Pok ,AcceptN,AcceptV )
2.2.接受者: 本地编号  >= N  :不响应或响应error



第二阶段:Accept同意请求

1.提议者(Proposer) ：半数以上的返回，发送 [N,V]提案的Accept同意请求给半数以上 接受者(Acceptor)
注意：V是收到的响应中编号最大的提案的value，如果响应中不包含任何提案，那么V由提议者(Proposer)决定。

2.接受者(Acceptor)收到请求： 
2.1.接受者: 没有接收过大于N的Prepare请求，就接受这个提案。
2.1.接受者: 接收过更大的Prepare请求，不响应或响应error。



如果响应过半，确定V被选定。
如果不过半， 重新发起Prepare请求

I.Learner(记录者) 学习的三种方案

II.Acceptor(接受者)接受提案发送给Learner(记录者)方式:
1.将该提案发送给 **所有Learner(记录者**) 。 
优点:快速获取被选定的value
缺点:通信次数为M*N

2.将该提案发送给 **主Learner(记录者)** ，主记录者通知其他Learner(记录者)。
优点:通信次数减少(M+N-1)
缺点:单点问题,主故障

3.将该提案发送给 **Learner(记录者)集合**，主记录者通知其他Learner(记录者)。
优点:集合节点越多，可靠性越好
缺点:网络通信复杂度越高。


I.Paxos算法的活性 (活锁问题)
多个Proposer(提议者) ,同时提出递增提案，陷入活锁死循环。

选取主Proposer(提议者):既能保证安全性，又能保证活性的分布式一致性算法。




raft算法
4：Raft
5：ZAB

https://www.cnblogs.com/yxy-ngu/p/12587341.html#_label1_2








# I. 一致性Hash算法原理

一致性hash(增加删除节点不会大面积影响)
https://www.jianshu.com/p/528ce5cd7e8f


II.定义等
目的是解决分布式缓存的问题
在移除或者添加一个服务器时，能够尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系。
一致性哈希解决了简单哈希算法在分布式哈希表( Distributed Hash Table，DHT) 中存在的动态伸缩等问题

II.工作原理
构造环
对存储节点的哈希值进行计算，其将存储空间抽象为一个环，将存储节点配置到环上。
环上所有的节点都有一个值

对数据进行哈希计算，按顺时针方向将其映射到离其最近的节点上去

当有节点出现故障离线时，按照算法的映射方法，受影响的仅仅为环上故障节点开始逆时针方向至下一个节点之间区间的数据对象，而这些对象本身就是映射到故障节点之上的。


当有节点增加时，比如，在节点A和B之间重新添加一个节点H，受影响的也仅仅是节点H逆时针遍历直到B之间的数据对象，将这些重新映射到H上即可


**II.讲解：构造环，上只有ABC三个节点**

**请求1来时：**
命中到环的A-B区间，就请求到B。
B-C区间，请求到B 。 
C-A区间，请求A节点。


**节点B挂了，AC正常**
命中到环的A-B区间，就请求到C。 只有C节点会受影响。

**新增H节点，ABC正常**
C-H区间，数据请求到H。 只有C节点会受影响


